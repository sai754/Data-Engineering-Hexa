{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90695b11-cb87-48e8-88d8-529e6702852f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.cp(\"file:/Workspace/Shared/csv_orders.csv\",\"dbfs:/FileStore/csv_orders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37270fde-e746-4ab0-8129-6dbc8740b049",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "#  Assignment: Creating a Complete ETL Pipeline using Delta Live Tables\n",
    "\n",
    "# Task 1\n",
    "schema = \"OrderID STRING, OrderDate STRING, CustomerID STRING, Product STRING, Quantity INT, Price INT\"\n",
    "\n",
    "df_orders = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schema).load(\"dbfs:/FileStore/csv_orders.csv\")\n",
    "\n",
    "# Transformations\n",
    "df_orders = df_orders.withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"Price\"))\n",
    "\n",
    "df_orders_filterd = df_orders.filter(col(\"Quantity\") > 1)\n",
    "\n",
    "# Write into delta\n",
    "df_orders_filterd.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/FileStore/delta/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b75b6b74-ef16-4437-ad4a-d497a40ca74d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta live table created\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "<html>\n",
       "  <style>\n",
       "<style>\n",
       "      html {\n",
       "        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,\n",
       "        Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,\n",
       "        Noto Color Emoji,FontAwesome;\n",
       "        font-size: 13;\n",
       "      }\n",
       "\n",
       "      .ansiout {\n",
       "        padding-bottom: 8px;\n",
       "      }\n",
       "\n",
       "      .createPipeline {\n",
       "        background-color: rgb(34, 114, 180);\n",
       "        color: white;\n",
       "        text-decoration: none;\n",
       "        padding: 4px 12px;\n",
       "        border-radius: 4px;\n",
       "        display: inline-block;\n",
       "      }\n",
       "\n",
       "      .createPipeline:hover {\n",
       "        background-color: #195487;\n",
       "      }\n",
       "\n",
       "      .tag {\n",
       "        border: none;\n",
       "        color: rgb(31, 39, 45);\n",
       "        padding: 2px 4px;\n",
       "        font-weight: 600;\n",
       "        background-color: rgba(93, 114, 131, 0.08);\n",
       "        border-radius: 4px;\n",
       "        margin-right: 0;\n",
       "        display: inline-block;\n",
       "        cursor: default;\n",
       "      }\n",
       "\n",
       "      table {\n",
       "        border-collapse: collapse;\n",
       "        font-size: 13px;\n",
       "      }\n",
       "\n",
       "      th {\n",
       "        text-align: left;\n",
       "        background-color: #F2F5F7;\n",
       "        padding-left: 8px;\n",
       "        padding-right: 8px;\n",
       "      }\n",
       "\n",
       "      tr {\n",
       "        border-bottom: solid;\n",
       "        border-bottom-color: #CDDAE5;\n",
       "        border-bottom-width: 1px;\n",
       "      }\n",
       "\n",
       "      td {\n",
       "        padding-left: 8px;\n",
       "        padding-right: 8px;\n",
       "      }\n",
       "\n",
       "      .dlt-label {\n",
       "        font-weight: bold;\n",
       "      }\n",
       "\n",
       "      ul {\n",
       "        list-style: circle;\n",
       "        padding-inline-start: 12px;\n",
       "      }\n",
       "\n",
       "      li {\n",
       "        padding-bottom: 4px;\n",
       "      }\n",
       "</style></style>\n",
       "  \n",
       "<div class=\"ansiout\">\n",
       "<span class='tag'>orders_table1</span> is defined as a\n",
       "<span class=\"dlt-label\">Delta Live Tables</span> dataset\n",
       " with schema: \n",
       "</div>\n",
       "\n",
       "  \n",
       "<div class=\"ansiout\">\n",
       "   <table>\n",
       "     <tbody>\n",
       "       <tr>\n",
       "         <th>Name</th>\n",
       "         <th>Type</th>\n",
       "       </tr>\n",
       "       \n",
       "<tr>\n",
       "   <td>OrderID</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>OrderDate</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>CustomerID</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>Product</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>Quantity</td>\n",
       "   <td>int</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>Price</td>\n",
       "   <td>int</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>TotalAmount</td>\n",
       "   <td>int</td>\n",
       "</tr>\n",
       "     </tbody>\n",
       "   </table>\n",
       "</div>\n",
       "\n",
       "  <div class =\"ansiout\">\n",
       "    To populate your table you must either:\n",
       "    <ul>\n",
       "      <li>\n",
       "        Run an existing pipeline using the\n",
       "        <span class=\"dlt-label\">Delta Live Tables</span> menu\n",
       "      </li>\n",
       "      <li>\n",
       "        Create a new pipeline: <a class='createPipeline' href=\"?o=2548899950097016#joblist/pipelines/create?initialSource=%2FUsers%2Fazuser2121_mml.local%40techademy.com%2FDLT-Exercise-17th&redirectNotebookId=4166212652284049\">Create Pipeline</a>\n",
       "      </li>\n",
       "    </ul>\n",
       "  <div>\n",
       "</html>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating dlt\n",
    "import dlt\n",
    "\n",
    "@dlt.table\n",
    "def orders_table1():\n",
    "    return spark.read.format(\"delta\").load(\"dbfs:/FileStore/delta/orders\")\n",
    "\n",
    "print(\"delta live table created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffbbc964-a6d6-4100-9ca9-2d1ce2a41446",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+-------+--------+-----+-----------+\n|OrderID| OrderDate|CustomerID|Product|Quantity|Price|TotalAmount|\n+-------+----------+----------+-------+--------+-----+-----------+\n|    101|2024-01-01|      C001| Laptop|       2| 1000|       2000|\n|    103|2024-01-03|      C003| Tablet|       3|  300|        900|\n|    105|2024-01-05|      C005|  Mouse|       5|   20|        100|\n+-------+----------+----------+-------+--------+-----+-----------+\n\nUpdation\n+-------+----------+----------+-------+--------+------+-----------+\n|OrderID| OrderDate|CustomerID|Product|Quantity| Price|TotalAmount|\n+-------+----------+----------+-------+--------+------+-----------+\n|    101|2024-01-01|      C001| Laptop|       2|1100.0|       2000|\n|    103|2024-01-03|      C003| Tablet|       3| 300.0|        900|\n|    105|2024-01-05|      C005|  Mouse|       5|  20.0|        100|\n+-------+----------+----------+-------+--------+------+-----------+\n\nDeleted\n+-------+----------+----------+-------+--------+------+-----------+\n|OrderID| OrderDate|CustomerID|Product|Quantity| Price|TotalAmount|\n+-------+----------+----------+-------+--------+------+-----------+\n|    101|2024-01-01|      C001| Laptop|       2|1100.0|       2000|\n|    103|2024-01-03|      C003| Tablet|       3| 300.0|        900|\n|    105|2024-01-05|      C005|  Mouse|       5|  20.0|        100|\n+-------+----------+----------+-------+--------+------+-----------+\n\n+-------+----------+----------+--------+--------+-----+-----------+\n|OrderID| OrderDate|CustomerID| Product|Quantity|Price|TotalAmount|\n+-------+----------+----------+--------+--------+-----+-----------+\n|    101|2024-01-01|      C001|  Laptop|       2| 1000|       2000|\n|    103|2024-01-03|      C003|  Tablet|       3|  300|        900|\n|    105|2024-01-05|      C005|   Mouse|       5|   20|        100|\n|    106|2024-01-06|      C006|Keyboard|       3|   50|        150|\n+-------+----------+----------+--------+--------+-----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Task 3 Operations on delta table\n",
    "\n",
    "# Read\n",
    "df_dltOrders = spark.read.format(\"delta\").load(\"dbfs:/FileStore/delta/orders\")\n",
    "df_dltOrders.show()\n",
    "\n",
    "# Update\n",
    "df_dltOrders = df_dltOrders.withColumn(\"Price\",\n",
    "    when(col(\"Product\") == \"Laptop\", col(\"Price\") * 1.10).otherwise(col(\"Price\")))\n",
    "\n",
    "print(\"Updation\")\n",
    "df_dltOrders.show()\n",
    "\n",
    "# Delete\n",
    "print(\"Deleted\")\n",
    "df_deleted =  df_dltOrders.filter(col(\"Quantity\") >= 2)\n",
    "df_deleted.show()\n",
    "\n",
    "# insert\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO delta.`dbfs:/FileStore/delta/orders`\n",
    "VALUES ('106', '2024-01-06', 'C006', 'Keyboard', 3, 50,150)\n",
    "\"\"\")\n",
    "\n",
    "updated_df = spark.read.format(\"delta\").load(\"dbfs:/FileStore/delta/orders\")\n",
    "updated_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "046f3dba-a9dc-480f-abac-bbd3f6f51fa1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.cp(\"file:/Workspace/Shared/csv_new_orders.csv\", \"dbfs:/FileStore/csv_new_orders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8ea9c2b-3759-490f-908b-3b19b7b1959a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+--------+--------+-----+-----------+\n|OrderID| OrderDate|CustomerID| Product|Quantity|Price|TotalAmount|\n+-------+----------+----------+--------+--------+-----+-----------+\n|    103|2024-01-03|      C003|  Tablet|       3|  300|        900|\n|    105|2024-01-05|      C005|   Mouse|       5|   20|        100|\n|    101|2024-01-10|      C001|  Laptop|       2| 1200|       2400|\n|    106|2024-01-12|      C006|Keyboard|       3|   50|        150|\n+-------+----------+----------+--------+--------+-----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Task 4 Merge\n",
    "df_new_orders = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/csv_new_orders.csv\")\n",
    "df_new_orders = df_new_orders.withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"Price\"))\n",
    "\n",
    "df_new_orders.createOrReplaceTempView(\"new_orders\")\n",
    "\n",
    "df_old_orders = spark.read.format(\"delta\").load(\"dbfs:/FileStore/delta/orders\")\n",
    "df_old_orders.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "          MERGE INTO orders as target \n",
    "          USING new_orders as source\n",
    "          ON target.OrderID = source.OrderID\n",
    "          WHEN MATCHED THEN\n",
    "          UPDATE SET target.OrderDate = source.OrderDate,\n",
    "                     target.CustomerID = source.CustomerID,\n",
    "                     target.Product = source.Product,\n",
    "                     target.Quantity = source.Quantity,\n",
    "                     target.Price = source.Price,\n",
    "                     target.TotalAmount = source.TotalAmount\n",
    "          WHEN NOT MATCHED THEN\n",
    "          INSERT (OrderID, OrderDate, CustomerID, Product, Quantity, Price, TotalAmount) \n",
    "          VALUES (source.OrderID, source.OrderDate, source.CustomerID, source.Product, source.Quantity, source.Price, source.TotalAmount)\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM orders\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bcdace4-4baf-4128-a97b-1ac98056ca7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+--------------------+--------------------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|version|          timestamp|          userId|            userName|           operation| operationParameters| job|          notebook|           clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n+-------+-------------------+----------------+--------------------+--------------------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|      0|2024-09-17 07:42:03|6822499424790006|azuser2121_mml.lo...|CREATE TABLE AS S...|{partitionBy -> [...|NULL|{4166212652284049}|0911-073432-r1idfcx3|       NULL|WriteSerializable|         true|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n+-------+-------------------+----------------+--------------------+--------------------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n\n+------+--------------------+--------------------+-----------+--------------------+--------------------+-------------------+----------------+-----------------+--------+-----------+--------------------+----------------+----------------+-----------------+--------------------+\n|format|                  id|                name|description|            location|           createdAt|       lastModified|partitionColumns|clusteringColumns|numFiles|sizeInBytes|          properties|minReaderVersion|minWriterVersion|    tableFeatures|          statistics|\n+------+--------------------+--------------------+-----------+--------------------+--------------------+-------------------+----------------+-----------------+--------+-----------+--------------------+----------------+----------------+-----------------+--------------------+\n| delta|d60f78cb-7a6d-4d8...|spark_catalog.def...|       NULL|dbfs:/user/hive/w...|2024-09-17 07:42:...|2024-09-17 07:42:03|              []|               []|       1|       1721|{delta.enableDele...|               3|               7|[deletionVectors]|{numRowsDeletedBy...|\n+------+--------------------+--------------------+-----------+--------------------+--------------------+-------------------+----------------+-----------------+--------+-----------+--------------------+----------------+----------------+-----------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Task 5\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS orders_delta_table USING DELTA AS SELECT * FROM orders\")\n",
    "\n",
    "# History\n",
    "spark.sql(\"DESCRIBE HISTORY orders_delta_table\").show()\n",
    "\n",
    "# Detail\n",
    "spark.sql(\"DESCRIBE DETAIL orders_delta_table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b510ba34-4572-4b06-8763-69f30bc7b871",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+--------+--------+-----+-----------+\n|OrderID| OrderDate|CustomerID| Product|Quantity|Price|TotalAmount|\n+-------+----------+----------+--------+--------+-----+-----------+\n|    103|2024-01-03|      C003|  Tablet|       3|  300|        900|\n|    105|2024-01-05|      C005|   Mouse|       5|   20|        100|\n|    101|2024-01-10|      C001|  Laptop|       2| 1200|       2400|\n|    106|2024-01-12|      C006|Keyboard|       3|   50|        150|\n+-------+----------+----------+--------+--------+-----+-----------+\n\n+-------+----------+----------+--------+--------+-----+-----------+\n|OrderID| OrderDate|CustomerID| Product|Quantity|Price|TotalAmount|\n+-------+----------+----------+--------+--------+-----+-----------+\n|    103|2024-01-03|      C003|  Tablet|       3|  300|        900|\n|    105|2024-01-05|      C005|   Mouse|       5|   20|        100|\n|    101|2024-01-10|      C001|  Laptop|       2| 1200|       2400|\n|    106|2024-01-12|      C006|Keyboard|       3|   50|        150|\n+-------+----------+----------+--------+--------+-----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Task 6 Time Travel\n",
    "\n",
    "spark.sql(\"SELECT * FROM orders_delta_table VERSION AS OF 0\").show()\n",
    "\n",
    "spark.sql(\"SELECT * FROM orders_delta_table TIMESTAMP AS OF '2024-09-17T07:42:03.000Z'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb02137c-3f05-4d92-8713-2647f67bb07f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[path: string]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 7 Optimize and vacuum\n",
    "\n",
    "spark.sql(\"OPTIMIZE orders_delta_table\")\n",
    "\n",
    "spark.sql(\"VACUUM orders_delta_table RETAIN 168 HOURS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d487c642-3820-4fd7-b298-9cba148ff826",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+-------+--------+-----+\n|OrderID| OrderDate|CustomerID|Product|Quantity|Price|\n+-------+----------+----------+-------+--------+-----+\n|    101|2024-01-01|      C001| Laptop|       2| 1000|\n|    102|2024-01-02|      C002|  Phone|       1|  500|\n|    103|2024-01-03|      C003| Tablet|       3|  300|\n|    104|2024-01-04|      C004|Monitor|       1|  150|\n|    105|2024-01-05|      C005|  Mouse|       5|   20|\n+-------+----------+----------+-------+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Task 8 convert parquet to delta\n",
    "\n",
    "csv_par_df = spark.read.csv(\"file:/Workspace/Shared/csv_orders.csv\",header=True,inferSchema=True)\n",
    "csv_par_df.write.format(\"parquet\").mode(\"overwrite\").save(\"dbfs:/FileStore/parquet_orders1\")\n",
    "\n",
    "# Load the Parquet files into a DataFrame\n",
    "parquet_df = spark.read.format(\"parquet\").load(\"dbfs:/FileStore/parquet_orders1/\")\n",
    "\n",
    "parquet_df.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/FileStore/delta/orders_delta_parquet1\")\n",
    "\n",
    "# Load the Delta table\n",
    "delta_df = spark.read.format(\"delta\").load(\"dbfs:/FileStore/delta/orders_delta_parquet1\")\n",
    "\n",
    "delta_df.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4166212652284056,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DLT-Exercise-17th",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
