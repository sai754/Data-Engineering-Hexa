{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c15df9e0-d2f3-4e37-8f84-3545b0f33ecf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.cp(\"file:/Workspace/Shared/csv_employees.csv\", \"dbfs:/FileStore/csv_employees.csv\")\n",
    "\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/json_products.json\", \"dbfs:/FileStore/json_products.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2c4bf28-aa55-480f-bd7b-094cf79b7ce4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-----------+------+\n|EmployeeID|EmployeeName| Department|JoiningDate|Salary|\n+----------+------------+-----------+-----------+------+\n|       101|        John|         HR| 2023-01-10| 50000|\n|       102|       Alice|    Finance| 2023-02-15| 70000|\n|       103|        Mark|Engineering| 2023-03-20| 85000|\n|       104|        Emma|      Sales| 2023-04-01| 55000|\n|       105|        Liam|  Marketing| 2023-05-12| 60000|\n+----------+------------+-----------+-----------+------+\n\n+---------+-----------+-----------+-----+\n|ProductID|ProductName|   Category|Price|\n+---------+-----------+-----------+-----+\n|     P101|     Laptop|Electronics| 1200|\n|     P102|      Phone|Electronics|  800|\n|     P103|     Tablet|Electronics|  600|\n|     P104|    Monitor|Electronics|  300|\n|     P105|      Mouse|Accessories|   25|\n+---------+-----------+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Load CSV and JSON datasets\n",
    "employee_df = spark.read.csv(\"dbfs:/FileStore/csv_employees.csv\", header=True, inferSchema=True)\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"ProductID\", StringType(), True),\n",
    "    StructField(\"ProductName\", StringType(), True),\n",
    "    StructField(\"Category\", StringType(), True),\n",
    "    StructField(\"Price\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "product_df = spark.read.schema(schema).json(\"dbfs:/FileStore/json_products.json\")\n",
    "\n",
    "# Delta table from dataframe\n",
    "employee_df.write.format(\"delta\").mode(\"overwrite\").save(\"/FileStore/delta/employees\")\n",
    "\n",
    "product_df.write.format(\"delta\").mode(\"overwrite\").save(\"/FileStore/delta/products\")\n",
    "\n",
    "# Delta table using SQL\n",
    "employee_df.createOrReplaceTempView(\"employees_view\")\n",
    "product_df.createOrReplaceTempView(\"products_view\")\n",
    "\n",
    "spark.sql(\"CREATE TABLE delta_employees USING DELTA AS SELECT * FROM employees_view\")\n",
    "\n",
    "spark.sql(\"CREATE TABLE delta_products USING DELTA AS SELECT * FROM products_view\")\n",
    "\n",
    "# Convert csv and json directly to delta\n",
    "employee_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"delta_employees_csv\")\n",
    "\n",
    "product_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"delta_products_json\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM delta_employees\").show()\n",
    "spark.sql(\"SELECT * FROM delta_products\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c65a4f29-00bb-4bd4-9e2c-8a7655c540d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.cp(\"file:/Workspace/Shared/csv_new_employees.csv\", \"dbfs:/FileStore/csv_new_employees.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e1e59ed-0542-42ff-a919-8e3003599c17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-----------+------+\n|EmployeeID|EmployeeName| Department|JoiningDate|Salary|\n+----------+------------+-----------+-----------+------+\n|       101|        John|         HR| 2023-01-10| 50000|\n|       103|        Mark|Engineering| 2023-03-20| 85000|\n|       104|        Emma|      Sales| 2023-04-01| 55000|\n|       105|        Liam|  Marketing| 2023-05-12| 60000|\n|       102|       Alice|    Finance| 2023-02-15| 75000|\n|       106|      Olivia|         HR| 2023-06-10| 65000|\n+----------+------------+-----------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Task 2\n",
    "# Merge and upsert \n",
    "new_employee_df = spark.read.csv(\"dbfs:/FileStore/csv_new_employees.csv\", header=True, inferSchema=True)\n",
    "\n",
    "employee_df = spark.read.format(\"delta\").load(\"/FileStore/delta/employees\")\n",
    "\n",
    "new_employee_df.createOrReplaceTempView(\"new_employees_view\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "           MERGE INTO delta_employees AS target\n",
    "           USING new_employees_view AS source\n",
    "           ON target.EmployeeID = source.EmployeeID\n",
    "           WHEN MATCHED THEN\n",
    "           UPDATE SET target.Salary = source.Salary,\n",
    "                      target.EmployeeName = source.EmployeeName,\n",
    "                      target.Department = source.Department,\n",
    "                      target.JoiningDate = source.JoiningDate\n",
    "           WHEN NOT MATCHED THEN\n",
    "           INSERT (EmployeeID, Salary, EmployeeName, Department, JoiningDate)\n",
    "           VALUES (source.EmployeeID, source.Salary, source.EmployeeName, source.Department, source.JoiningDate) \n",
    " \"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM employees_view\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41e453da-45d9-4221-a235-3723be76c859",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+--------------------+--------------------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|version|          timestamp|          userId|            userName|           operation| operationParameters| job|          notebook|           clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n+-------+-------------------+----------------+--------------------+--------------------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|      2|2024-09-17 04:26:08|6822499424790006|azuser2121_mml.lo...|            OPTIMIZE|{predicate -> [],...|NULL|{4166212652284004}|0911-073432-r1idfcx3|          1|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|      1|2024-09-17 04:26:05|6822499424790006|azuser2121_mml.lo...|               MERGE|{predicate -> [\"(...|NULL|{4166212652284004}|0911-073432-r1idfcx3|          0|WriteSerializable|        false|{numTargetRowsCop...|        NULL|Databricks-Runtim...|\n|      0|2024-09-17 04:09:24|6822499424790006|azuser2121_mml.lo...|CREATE TABLE AS S...|{partitionBy -> [...|NULL|{4166212652284004}|0911-073432-r1idfcx3|       NULL|WriteSerializable|         true|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n+-------+-------------------+----------------+--------------------+--------------------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n\n+----------+------------+-----------+-----------+------+\n|EmployeeID|EmployeeName| Department|JoiningDate|Salary|\n+----------+------------+-----------+-----------+------+\n|       101|        John|         HR| 2023-01-10| 50000|\n|       102|       Alice|    Finance| 2023-02-15| 70000|\n|       103|        Mark|Engineering| 2023-03-20| 85000|\n|       104|        Emma|      Sales| 2023-04-01| 55000|\n|       105|        Liam|  Marketing| 2023-05-12| 60000|\n+----------+------------+-----------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Task 3\n",
    "# Internals\n",
    "spark.sql(\"DESCRIBE DETAIL delta_employees\")\n",
    "\n",
    "# Transaction History\n",
    "spark.sql(\"DESCRIBE HISTORY delta_employees\").show()\n",
    "\n",
    "# Time Travel\n",
    "spark.sql(\"SELECT * FROM delta_employees VERSION AS OF 0\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ac9559f-9722-497d-9e2a-8b4a02e314e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,clusteringStats:struct<inputZCubeFiles:struct<numFiles:bigint,size:bigint>,inputOtherFiles:struct<numFiles:bigint,size:bigint>,inputNumZCubes:bigint,mergedFiles:struct<numFiles:bigint,size:bigint>,numOutputZCubes:bigint>,numBins:bigint,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint,totalTaskExecutionTimeMs:bigint,skippedArchivedFiles:bigint,clusteringMetrics:struct<sizeOfTableInBytesBeforeLazyClustering:bigint,isNewMetadataCreated:boolean,isPOTriggered:boolean,numFilesSkippedWithoutStats:bigint,numFilesClassifiedToIntermediateNodes:bigint,sizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,logicalSizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,numFilesClassifiedToLeafNodes:bigint,sizeOfFilesClassifiedToLeafNodesInBytes:bigint,logicalSizeOfFilesClassifiedToLeafNodesInBytes:bigint,numThreadsForClassifier:int,clusterThresholdStrategy:string,minFileSize:bigint,maxFileSize:bigint,nodeMinNumFilesToCompact:bigint,numIdealFiles:bigint,numClusteringTasksPlanned:int,numCompactionTasksPlanned:int,numOptimizeBatchesPlanned:int,numLeafNodesExpanded:bigint,numLeafNodesClustered:bigint,numGetFilesForNodeCalls:bigint,numSamplingJobs:bigint,numLeafNodesCompacted:bigint,numIntermediateNodesCompacted:bigint,totalSizeOfDataToCompactInBytes:bigint,totalLogicalSizeOfDataToCompactInBytes:bigint,numIntermediateNodesClustered:bigint,numFilesSkippedAfterExpansion:bigint,totalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalLogicalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalSizeOfDataToRewriteInBytes:bigint,totalLogicalSizeOfDataToRewriteInBytes:bigint,timeMetrics:struct<classifierTimeMs:bigint,optimizerTimeMs:bigint,metadataLoadTimeMs:bigint,totalGetFilesForNodeCallsTimeMs:bigint,totalSamplingTimeMs:bigint,metadataCreationTimeMs:bigint>,maxOptimizeBatchesInParallel:bigint,currentIteration:int,maxIterations:int,clusteringStrategy:string>>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 4 Optimizing\n",
    "spark.sql(\"OPTIMIZE delta_employees\")\n",
    "\n",
    "# Z ordering\n",
    "spark.sql(\"OPTIMIZE delta_employees ZORDER BY (Department)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7c342df-48d4-4cc1-8b24-b4d3d32d94e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[path: string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vacuuming\n",
    "spark.sql(\"VACUUM delta_employees RETAIN 168 HOURS\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DeltaLakeExercises-1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
