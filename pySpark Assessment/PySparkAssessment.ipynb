{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1l8S_Fb8G83w",
        "outputId": "5a84f3ba-4ede-45fd-985c-414e2eaab0bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=7ee74b154a8ac8b198b25b6cc2700536b008a9dea6cedd5653ecb475da78eec5\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Ecommerce\").getOrCreate()\n",
        "\n",
        "ecommerce_df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/content/sample_data/ecommerce_data.csv\")\n",
        "\n",
        "# 1. Calculate the Total Revenue per Category\n",
        "\n",
        "total_revenue_df = ecommerce_df.withColumn(\"revenue\", (col(\"price\") * col(\"quantity\")) * (1-col(\"discount_percentage\")/100)).groupBy(\"category\").agg(F.sum(\"revenue\").alias(\"total_revenue\"))\n",
        "total_revenue_df.show()\n",
        "\n",
        "# 2. Filter Transactions with a Discount Greater Than 10%\n",
        "\n",
        "high_discount_df = ecommerce_df.filter(col(\"discount_percentage\") > 10)\n",
        "high_discount_df.show()\n",
        "\n",
        "# 3. Find the Most Expensive Product Sold\n",
        "\n",
        "expensive_df = ecommerce_df.orderBy(col(\"price\").desc()).limit(1)\n",
        "expensive_df.show()\n",
        "\n",
        "# 4. Calculate the Average Quantity of Products Sold per Category\n",
        "\n",
        "avg_quantity_df = ecommerce_df.groupBy(\"category\").agg(F.avg(\"quantity\").alias(\"average_quantity\"))\n",
        "avg_quantity_df.show()\n",
        "\n",
        "# 5. Identify Customers Who Purchased More Than One Product in single transaction\n",
        "\n",
        "high_buy_df = ecommerce_df.filter(col(\"quantity\")>1)\n",
        "high_buy_df.show()\n",
        "\n",
        "# 6. Find the Top 3 Highest Revenue Transactions\n",
        "\n",
        "top_3_highest_df = ecommerce_df.withColumn(\"revenue\", (col(\"price\") * col(\"quantity\")) * (1-col(\"discount_percentage\")/100)).orderBy(col(\"revenue\").desc()).limit(3)\n",
        "top_3_highest_df.show()\n",
        "\n",
        "\n",
        "# 7. Calculate the Total Number of Transactions per Day\n",
        "\n",
        "transaction_per_day = ecommerce_df.groupBy(\"transaction_date\").agg(F.count(\"*\").alias(\"transaction_count\"))\n",
        "transaction_per_day.show()\n",
        "\n",
        "# 8. Find the Customer Who Spent the Most Money\n",
        "\n",
        "high_customer_df = ecommerce_df.withColumn(\"total_spent\",(col(\"price\") * col(\"quantity\")) * (1-col(\"discount_percentage\")/100)).groupBy(\"customer_id\").agg(F.sum(\"total_spent\").alias(\"total_spent\")) \\\n",
        "                  .orderBy(col(\"total_spent\").desc()).limit(1)\n",
        "high_customer_df.show()\n",
        "\n",
        "# 9. Calculate the Average Discount Given per Product Category\n",
        "\n",
        "avg_discount_df = ecommerce_df.groupBy(\"category\").agg(F.avg(\"discount_percentage\").alias(\"average_discount\"))\n",
        "avg_discount_df.show()\n",
        "\n",
        "# 10. Create a New Column for Final Price After Discount\n",
        "\n",
        "ecommerce_df = ecommerce_df.withColumn(\"final_price\", col(\"price\") - (col(\"price\") * col(\"discount_percentage\") / 100))\n",
        "ecommerce_df.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dhO3BgEIDsV",
        "outputId": "9f5ca8ee-6e13-4631-bcc8-15f9d45a3a9f"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-------------+\n",
            "|      category|total_revenue|\n",
            "+--------------+-------------+\n",
            "|       Fashion|        168.0|\n",
            "|   Electronics|       2950.0|\n",
            "|         Books|         80.0|\n",
            "|Home Appliance|        756.0|\n",
            "+--------------+-------------+\n",
            "\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+\n",
            "|transaction_id|customer_id|     product|      category|price|quantity|discount_percentage|transaction_date|\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+\n",
            "|             4|        104|     Blender|Home Appliance|  150|       1|                 15|      2023-08-03|\n",
            "|             6|        105|       Shoes|       Fashion|   60|       1|                 20|      2023-08-04|\n",
            "|             7|        106|Refrigerator|Home Appliance|  800|       1|                 25|      2023-08-05|\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+\n",
            "\n",
            "+--------------+-----------+-------+-----------+-----+--------+-------------------+----------------+\n",
            "|transaction_id|customer_id|product|   category|price|quantity|discount_percentage|transaction_date|\n",
            "+--------------+-----------+-------+-----------+-----+--------+-------------------+----------------+\n",
            "|             1|        101| Laptop|Electronics| 1000|       1|                 10|      2023-08-01|\n",
            "+--------------+-----------+-------+-----------+-----+--------+-------------------+----------------+\n",
            "\n",
            "+--------------+----------------+\n",
            "|      category|average_quantity|\n",
            "+--------------+----------------+\n",
            "|       Fashion|             2.0|\n",
            "|   Electronics|            1.75|\n",
            "|         Books|             4.0|\n",
            "|Home Appliance|             1.0|\n",
            "+--------------+----------------+\n",
            "\n",
            "+--------------+-----------+----------+-----------+-----+--------+-------------------+----------------+\n",
            "|transaction_id|customer_id|   product|   category|price|quantity|discount_percentage|transaction_date|\n",
            "+--------------+-----------+----------+-----------+-----+--------+-------------------+----------------+\n",
            "|             2|        102|Smartphone|Electronics|  700|       2|                  5|      2023-08-01|\n",
            "|             3|        103|     Shirt|    Fashion|   40|       3|                  0|      2023-08-02|\n",
            "|             5|        101|Headphones|Electronics|  100|       2|                 10|      2023-08-03|\n",
            "|             8|        107|      Book|      Books|   20|       4|                  0|      2023-08-05|\n",
            "|            10|        102|    Tablet|Electronics|  300|       2|                 10|      2023-08-06|\n",
            "+--------------+-----------+----------+-----------+-----+--------+-------------------+----------------+\n",
            "\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+-------+\n",
            "|transaction_id|customer_id|     product|      category|price|quantity|discount_percentage|transaction_date|revenue|\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+-------+\n",
            "|             2|        102|  Smartphone|   Electronics|  700|       2|                  5|      2023-08-01| 1330.0|\n",
            "|             1|        101|      Laptop|   Electronics| 1000|       1|                 10|      2023-08-01|  900.0|\n",
            "|             7|        106|Refrigerator|Home Appliance|  800|       1|                 25|      2023-08-05|  600.0|\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+-------+\n",
            "\n",
            "+----------------+-----------------+\n",
            "|transaction_date|transaction_count|\n",
            "+----------------+-----------------+\n",
            "|      2023-08-03|                2|\n",
            "|      2023-08-06|                2|\n",
            "|      2023-08-01|                2|\n",
            "|      2023-08-05|                2|\n",
            "|      2023-08-04|                1|\n",
            "|      2023-08-02|                1|\n",
            "+----------------+-----------------+\n",
            "\n",
            "+-----------+-----------+\n",
            "|customer_id|total_spent|\n",
            "+-----------+-----------+\n",
            "|        102|     1870.0|\n",
            "+-----------+-----------+\n",
            "\n",
            "+--------------+----------------+\n",
            "|      category|average_discount|\n",
            "+--------------+----------------+\n",
            "|       Fashion|            10.0|\n",
            "|   Electronics|            8.75|\n",
            "|         Books|             0.0|\n",
            "|Home Appliance|            15.0|\n",
            "+--------------+----------------+\n",
            "\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+-----------+\n",
            "|transaction_id|customer_id|     product|      category|price|quantity|discount_percentage|transaction_date|final_price|\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+-----------+\n",
            "|             1|        101|      Laptop|   Electronics| 1000|       1|                 10|      2023-08-01|      900.0|\n",
            "|             2|        102|  Smartphone|   Electronics|  700|       2|                  5|      2023-08-01|      665.0|\n",
            "|             3|        103|       Shirt|       Fashion|   40|       3|                  0|      2023-08-02|       40.0|\n",
            "|             4|        104|     Blender|Home Appliance|  150|       1|                 15|      2023-08-03|      127.5|\n",
            "|             5|        101|  Headphones|   Electronics|  100|       2|                 10|      2023-08-03|       90.0|\n",
            "|             6|        105|       Shoes|       Fashion|   60|       1|                 20|      2023-08-04|       48.0|\n",
            "|             7|        106|Refrigerator|Home Appliance|  800|       1|                 25|      2023-08-05|      600.0|\n",
            "|             8|        107|        Book|         Books|   20|       4|                  0|      2023-08-05|       20.0|\n",
            "|             9|        108|     Toaster|Home Appliance|   30|       1|                  5|      2023-08-06|       28.5|\n",
            "|            10|        102|      Tablet|   Electronics|  300|       2|                 10|      2023-08-06|      270.0|\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "spark = SparkSession.builder.appName(\"BankingTransaction\").getOrCreate()\n",
        "\n",
        "banking_df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/content/sample_data/bank_transaction.csv\")\n",
        "\n",
        "# 1. Calculate the Total Deposit and Withdrawal Amounts\n",
        "\n",
        "amounts_df = banking_df.groupBy(\"transaction_type\").agg(F.sum(\"amount\").alias(\"total_amount\"))\n",
        "amounts_df.show()\n",
        "\n",
        "# 2. Filter Transactions Greater Than $3,000\n",
        "\n",
        "high_transaction = banking_df.filter(col(\"amount\") > 3000)\n",
        "high_transaction.show()\n",
        "\n",
        "# 3. Find the Largest Deposit Made\n",
        "\n",
        "largest_deposit = banking_df.filter(col(\"transaction_type\") == \"Deposit\").orderBy(col(\"amount\").desc()).limit(1)\n",
        "largest_deposit.show()\n",
        "\n",
        "# 4. Calculate the Average Transaction Amount for Each Transaction Type\n",
        "\n",
        "avg_transaction_df = banking_df.groupBy(\"transaction_type\").agg(F.avg(\"amount\").alias(\"average_amount\"))\n",
        "avg_transaction_df.show()\n",
        "\n",
        "# 5. Find Customers Who Made Both Deposits and Withdrawals\n",
        "\n",
        "deposit_customers = banking_df.filter(col(\"transaction_type\") == \"Deposit\").select(\"customer_id\").distinct()\n",
        "withdrawal_customers = banking_df.filter(col(\"transaction_type\") == \"Withdrawal\").select(\"customer_id\").distinct()\n",
        "\n",
        "deposit_customers.intersect(withdrawal_customers).show()\n",
        "\n",
        "# 6. Calculate the Total Amount of Transactions per Day\n",
        "\n",
        "total_transaction_perday = banking_df.groupBy(\"transaction_date\").agg(F.sum(\"amount\").alias(\"total_amount\"))\n",
        "total_transaction_perday.show()\n",
        "\n",
        "# 7. Find the Customer with the Highest Total Withdrawal\n",
        "\n",
        "highest_withdrawal = banking_df.filter(col(\"transaction_type\") == \"Withdrawal\").groupBy(\"customer_id\").agg(F.sum(\"amount\").alias(\"total_withdrawn\")) \\\n",
        "          .orderBy(col(\"total_withdrawn\").desc()).limit(1)\n",
        "\n",
        "highest_withdrawal.show()\n",
        "\n",
        "# 8. Calculate the Number of Transactions for Each Customer\n",
        "\n",
        "transaction_per_customer = banking_df.groupBy(\"customer_id\").agg(F.count(\"transaction_id\").alias(\"transaction_count\"))\n",
        "transaction_per_customer.show()\n",
        "\n",
        "# 9. Find All Transactions That Occurred on the Same Day as a Withdrawal Greater\n",
        "#  Than $1,000\n",
        "\n",
        "withdrawal_dates = banking_df.filter((col(\"transaction_type\") == \"Withdrawal\") & (col(\"amount\") > 1000)) \\\n",
        "                             .select(\"transaction_date\").distinct()\n",
        "\n",
        "banking_df.join(withdrawal_dates, on=\"transaction_date\").show()\n",
        "\n",
        "# 10. Create a New Column to Classify Transactions as \"High\" or \"Low\" Value\n",
        "\n",
        "banking_df = banking_df.withColumn(\"transaction_value\", F.when(col(\"amount\") > 5000, \"High\").otherwise(\"Low\"))\n",
        "\n",
        "banking_df.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_hloOhhPmtg",
        "outputId": "cf6406a5-c994-4434-d009-687824768967"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+------------+\n",
            "|transaction_type|total_amount|\n",
            "+----------------+------------+\n",
            "|         Deposit|       24500|\n",
            "|      Withdrawal|        7700|\n",
            "+----------------+------------+\n",
            "\n",
            "+--------------+-----------+----------------+------+-------------------+\n",
            "|transaction_id|customer_id|transaction_type|amount|   transaction_date|\n",
            "+--------------+-----------+----------------+------+-------------------+\n",
            "|             1|        201|         Deposit|  5000|2023-09-01 00:00:00|\n",
            "|             5|        204|         Deposit| 10000|2023-09-03 00:00:00|\n",
            "|             9|        203|         Deposit|  4000|2023-09-05 00:00:00|\n",
            "+--------------+-----------+----------------+------+-------------------+\n",
            "\n",
            "+--------------+-----------+----------------+------+-------------------+\n",
            "|transaction_id|customer_id|transaction_type|amount|   transaction_date|\n",
            "+--------------+-----------+----------------+------+-------------------+\n",
            "|             5|        204|         Deposit| 10000|2023-09-03 00:00:00|\n",
            "+--------------+-----------+----------------+------+-------------------+\n",
            "\n",
            "+----------------+--------------+\n",
            "|transaction_type|average_amount|\n",
            "+----------------+--------------+\n",
            "|         Deposit|        4900.0|\n",
            "|      Withdrawal|        1540.0|\n",
            "+----------------+--------------+\n",
            "\n",
            "+-----------+\n",
            "|customer_id|\n",
            "+-----------+\n",
            "|        202|\n",
            "|        204|\n",
            "|        201|\n",
            "+-----------+\n",
            "\n",
            "+-------------------+------------+\n",
            "|   transaction_date|total_amount|\n",
            "+-------------------+------------+\n",
            "|2023-09-02 00:00:00|        4500|\n",
            "|2023-09-03 00:00:00|       10500|\n",
            "|2023-09-04 00:00:00|        3200|\n",
            "|2023-09-05 00:00:00|        7000|\n",
            "|2023-09-01 00:00:00|        7000|\n",
            "+-------------------+------------+\n",
            "\n",
            "+-----------+---------------+\n",
            "|customer_id|total_withdrawn|\n",
            "+-----------+---------------+\n",
            "|        204|           3000|\n",
            "+-----------+---------------+\n",
            "\n",
            "+-----------+-----------------+\n",
            "|customer_id|transaction_count|\n",
            "+-----------+-----------------+\n",
            "|        206|                1|\n",
            "|        205|                1|\n",
            "|        202|                2|\n",
            "|        203|                2|\n",
            "|        204|                2|\n",
            "|        201|                2|\n",
            "+-----------+-----------------+\n",
            "\n",
            "+-------------------+--------------+-----------+----------------+------+\n",
            "|   transaction_date|transaction_id|customer_id|transaction_type|amount|\n",
            "+-------------------+--------------+-----------+----------------+------+\n",
            "|2023-09-01 00:00:00|             1|        201|         Deposit|  5000|\n",
            "|2023-09-01 00:00:00|             2|        202|      Withdrawal|  2000|\n",
            "|2023-09-02 00:00:00|             3|        203|         Deposit|  3000|\n",
            "|2023-09-02 00:00:00|             4|        201|      Withdrawal|  1500|\n",
            "|2023-09-05 00:00:00|             9|        203|         Deposit|  4000|\n",
            "|2023-09-05 00:00:00|            10|        204|      Withdrawal|  3000|\n",
            "+-------------------+--------------+-----------+----------------+------+\n",
            "\n",
            "+--------------+-----------+----------------+------+-------------------+-----------------+\n",
            "|transaction_id|customer_id|transaction_type|amount|   transaction_date|transaction_value|\n",
            "+--------------+-----------+----------------+------+-------------------+-----------------+\n",
            "|             1|        201|         Deposit|  5000|2023-09-01 00:00:00|              Low|\n",
            "|             2|        202|      Withdrawal|  2000|2023-09-01 00:00:00|              Low|\n",
            "|             3|        203|         Deposit|  3000|2023-09-02 00:00:00|              Low|\n",
            "|             4|        201|      Withdrawal|  1500|2023-09-02 00:00:00|              Low|\n",
            "|             5|        204|         Deposit| 10000|2023-09-03 00:00:00|             High|\n",
            "|             6|        205|      Withdrawal|   500|2023-09-03 00:00:00|              Low|\n",
            "|             7|        202|         Deposit|  2500|2023-09-04 00:00:00|              Low|\n",
            "|             8|        206|      Withdrawal|   700|2023-09-04 00:00:00|              Low|\n",
            "|             9|        203|         Deposit|  4000|2023-09-05 00:00:00|              Low|\n",
            "|            10|        204|      Withdrawal|  3000|2023-09-05 00:00:00|              Low|\n",
            "+--------------+-----------+----------------+------+-------------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "spark = SparkSession.builder.appName(\"HealthFitness\").getOrCreate()\n",
        "\n",
        "health_df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/content/sample_data/health_fitness.csv\")\n",
        "\n",
        "# 1. Find the Total Steps Taken by Each User\n",
        "\n",
        "steps_user_df = health_df.groupBy(\"user_id\").agg(F.sum(\"steps\").alias(\"total_steps\"))\n",
        "steps_user_df.show()\n",
        "\n",
        "# 2. Filter Days with More Than 10,000 Steps\n",
        "\n",
        "high_steps = health_df.filter(col(\"steps\")>10000).select(\"date\", \"user_id\")\n",
        "high_steps.show()\n",
        "\n",
        "# 3. Calculate the Average Calories Burned by Workout Type\n",
        "\n",
        "avg_calories = health_df.groupBy(\"workout_type\").agg(F.avg(\"calories_burned\").alias(\"avg_calories\"))\n",
        "\n",
        "avg_calories.show()\n",
        "\n",
        "# 4. Identify the Day with the Most Steps for Each User\n",
        "\n",
        "max_user_steps = health_df.groupBy(\"user_id\").agg(F.max(\"steps\").alias(\"max_steps\"))\n",
        "\n",
        "max_user_steps_dates = max_user_steps.join(health_df,\"user_id\").filter(col(\"steps\")==col(\"max_steps\")).select(\"user_id\",\"date\",\"steps\")\n",
        "max_user_steps_dates.show()\n",
        "\n",
        "# 5. Find Users Who Burned More Than 600 Calories on Any Day\n",
        "\n",
        "high_calories_burned = health_df.filter(col(\"calories_burned\") > 600)\n",
        "high_calories_burned.show()\n",
        "\n",
        "# 6. Calculate the Average Hours of Sleep per User\n",
        "\n",
        "avg_sleep_hrs = health_df.groupBy(\"user_id\").agg(F.avg(\"hours_of_sleep\").alias(\"avg_sleep\"))\n",
        "avg_sleep_hrs.show()\n",
        "\n",
        "# 7. Find the Total Calories Burned per Day\n",
        "\n",
        "calories_burned_perday = health_df.groupBy(\"date\").agg(F.sum(\"calories_burned\").alias(\"total_calories\"))\n",
        "calories_burned_perday.show()\n",
        "\n",
        "# 8. Identify Users Who Did Different Types of Workouts\n",
        "\n",
        "different_workout = health_df.groupBy(\"user_id\").agg(F.countDistinct(\"workout_type\").alias(\"workout_types\")).filter(col(\"workout_types\") > 1)\n",
        "different_workout.show()\n",
        "\n",
        "# 9. Calculate the Total Number of Workouts per User\n",
        "\n",
        "total_workout_peruser = health_df.groupBy(\"user_id\").agg(F.count(\"workout_type\").alias(\"total_workouts\"))\n",
        "total_workout_peruser.show()\n",
        "\n",
        "# 10. Create a New Column for \"Active\" Days\n",
        "\n",
        "health_df = health_df.withColumn(\"active_day\", F.when(col(\"steps\") > 10000, \"Active\").otherwise(\"Inactive\"))\n",
        "health_df.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsZbksCoWrXR",
        "outputId": "4c7b3e22-3cbb-4efe-91e5-2eeee187ad56"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+\n",
            "|user_id|total_steps|\n",
            "+-------+-----------+\n",
            "|      1|      35000|\n",
            "|      3|      45000|\n",
            "|      2|      29500|\n",
            "+-------+-----------+\n",
            "\n",
            "+----------+-------+\n",
            "|      date|user_id|\n",
            "+----------+-------+\n",
            "|2023-09-01|      1|\n",
            "|2023-09-01|      3|\n",
            "|2023-09-02|      3|\n",
            "|2023-09-03|      1|\n",
            "|2023-09-03|      2|\n",
            "|2023-09-03|      3|\n",
            "+----------+-------+\n",
            "\n",
            "+------------+-----------------+\n",
            "|workout_type|     avg_calories|\n",
            "+------------+-----------------+\n",
            "|    Strength|            500.0|\n",
            "|        Yoga|573.3333333333334|\n",
            "|      Cardio|            537.5|\n",
            "+------------+-----------------+\n",
            "\n",
            "+-------+----------+-----+\n",
            "|user_id|      date|steps|\n",
            "+-------+----------+-----+\n",
            "|      1|2023-09-03|13000|\n",
            "|      2|2023-09-03|12000|\n",
            "|      3|2023-09-03|16000|\n",
            "+-------+----------+-----+\n",
            "\n",
            "+-------+----------+-----+---------------+--------------+------------+\n",
            "|user_id|      date|steps|calories_burned|hours_of_sleep|workout_type|\n",
            "+-------+----------+-----+---------------+--------------+------------+\n",
            "|      3|2023-09-01|15000|            650|           8.0|        Yoga|\n",
            "|      3|2023-09-03|16000|            700|           7.0|      Cardio|\n",
            "+-------+----------+-----+---------------+--------------+------------+\n",
            "\n",
            "+-------+-----------------+\n",
            "|user_id|        avg_sleep|\n",
            "+-------+-----------------+\n",
            "|      1|              7.0|\n",
            "|      3|              7.5|\n",
            "|      2|6.666666666666667|\n",
            "+-------+-----------------+\n",
            "\n",
            "+----------+--------------+\n",
            "|      date|total_calories|\n",
            "+----------+--------------+\n",
            "|2023-09-03|          1770|\n",
            "|2023-09-01|          1550|\n",
            "|2023-09-02|          1550|\n",
            "+----------+--------------+\n",
            "\n",
            "+-------+-------------+\n",
            "|user_id|workout_types|\n",
            "+-------+-------------+\n",
            "|      1|            2|\n",
            "|      3|            3|\n",
            "|      2|            3|\n",
            "+-------+-------------+\n",
            "\n",
            "+-------+--------------+\n",
            "|user_id|total_workouts|\n",
            "+-------+--------------+\n",
            "|      1|             3|\n",
            "|      3|             3|\n",
            "|      2|             3|\n",
            "+-------+--------------+\n",
            "\n",
            "+-------+----------+-----+---------------+--------------+------------+----------+\n",
            "|user_id|      date|steps|calories_burned|hours_of_sleep|workout_type|active_day|\n",
            "+-------+----------+-----+---------------+--------------+------------+----------+\n",
            "|      1|2023-09-01|12000|            500|           7.0|      Cardio|    Active|\n",
            "|      2|2023-09-01| 8000|            400|           6.5|    Strength|  Inactive|\n",
            "|      3|2023-09-01|15000|            650|           8.0|        Yoga|    Active|\n",
            "|      1|2023-09-02|10000|            450|           6.0|      Cardio|  Inactive|\n",
            "|      2|2023-09-02| 9500|            500|           7.0|      Cardio|  Inactive|\n",
            "|      3|2023-09-02|14000|            600|           7.5|    Strength|    Active|\n",
            "|      1|2023-09-03|13000|            550|           8.0|        Yoga|    Active|\n",
            "|      2|2023-09-03|12000|            520|           6.5|        Yoga|    Active|\n",
            "|      3|2023-09-03|16000|            700|           7.0|      Cardio|    Active|\n",
            "+-------+----------+-----+---------------+--------------+------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark = SparkSession.builder.appName(\"MusicStreaming\").getOrCreate()\n",
        "\n",
        "music_df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/content/sample_data/music_streaming.csv\")\n",
        "\n",
        "# 1. Calculate the Total Listening Time for Each User\n",
        "\n",
        "total_listening_time = music_df.groupBy(\"user_id\").agg(F.sum(\"duration_seconds\").alias(\"total_listening_time\"))\n",
        "total_listening_time.show()\n",
        "\n",
        "# 2. Filter Songs Streamed for More Than 200 Seconds\n",
        "\n",
        "high_duration = music_df.filter(col(\"duration_seconds\") > 200)\n",
        "high_duration.show()\n",
        "\n",
        "# 3. Find the Most Popular Artist (by Total Streams)\n",
        "\n",
        "popular_artist = music_df.groupBy(\"artist\").agg(F.count(\"*\").alias(\"total_streams\")).orderBy(col(\"total_streams\").desc()).limit(1)\n",
        "popular_artist.show()\n",
        "\n",
        "# 4. Identify the Song with the Longest Duration\n",
        "\n",
        "longest_duration_song = music_df.orderBy(col(\"duration_seconds\").desc()).select(\"song_title\",\"artist\",\"duration_seconds\").limit(1)\n",
        "longest_duration_song.show()\n",
        "\n",
        "# 5. Calculate the Average Song Duration by Artist\n",
        "\n",
        "avg_artist_duration = music_df.groupBy(\"artist\").agg(F.avg(\"duration_seconds\").alias(\"average_duration\"))\n",
        "avg_artist_duration.show()\n",
        "\n",
        "# 6. Find the Top 3 Most Streamed Songs per User\n",
        "\n",
        "grouped_df = music_df.groupBy(\"user_id\",\"song_title\").agg(F.count(\"*\").alias(\"play_count\"))\n",
        "\n",
        "window_spec = Window.partitionBy(\"user_id\").orderBy(col(\"play_count\").desc())\n",
        "\n",
        "ranked_df = grouped_df.withColumn(\"rank\",F.row_number().over(window_spec))\n",
        "\n",
        "top_3_df = ranked_df.filter(col(\"rank\") <=3).orderBy(col(\"user_id\"),col(\"rank\"))\n",
        "top_3_df.show()\n",
        "\n",
        "# 7. Calculate the Total Number of Streams per Day\n",
        "\n",
        "streams_per_day = music_df.withColumn(\"stream_date\",F.to_date(\"streaming_time\")).groupBy(\"stream_date\").agg(F.count(\"*\").alias(\"total_streams\"))\n",
        "streams_per_day.show()\n",
        "\n",
        "# 8. Identify Users Who Streamed Songs from More Than One Artist\n",
        "\n",
        "more_than_oneArtist = music_df.groupBy(\"user_id\").agg(F.countDistinct(\"artist\").alias(\"artist_count\")).filter(col(\"artist_count\")>1)\n",
        "more_than_oneArtist.show()\n",
        "\n",
        "# 9. Calculate the Total Streams for Each Location\n",
        "\n",
        "streams_per_location = music_df.groupBy(\"location\").agg(F.count(\"*\").alias(\"total_streams\"))\n",
        "streams_per_location.show()\n",
        "\n",
        "# 10. Create a New Column to Classify Long and Short Songs\n",
        "\n",
        "music_df = music_df.withColumn(\"song_length\", F.when(col(\"duration_seconds\") > 200, \"Long\").otherwise(\"Short\"))\n",
        "music_df.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lJHNnwQba6w",
        "outputId": "9c598130-03ac-414b-f370-ffb93945cdaf"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+\n",
            "|user_id|total_listening_time|\n",
            "+-------+--------------------+\n",
            "|      1|                 630|\n",
            "|      3|                 610|\n",
            "|      2|                 680|\n",
            "+-------+--------------------+\n",
            "\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+\n",
            "|user_id|     song_title|    artist|duration_seconds|     streaming_time|   location|\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+\n",
            "|      2|   Shape of You|Ed Sheeran|             240|2023-09-01 09:20:00|Los Angeles|\n",
            "|      1|        Starboy|The Weeknd|             220|2023-09-01 11:00:00|   New York|\n",
            "|      2|        Perfect|Ed Sheeran|             250|2023-09-01 12:15:00|Los Angeles|\n",
            "|      1|Save Your Tears|The Weeknd|             210|2023-09-02 09:00:00|   New York|\n",
            "|      3|      New Rules|  Dua Lipa|             230|2023-09-02 11:00:00|     London|\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+\n",
            "\n",
            "+--------+-------------+\n",
            "|  artist|total_streams|\n",
            "+--------+-------------+\n",
            "|Dua Lipa|            3|\n",
            "+--------+-------------+\n",
            "\n",
            "+----------+----------+----------------+\n",
            "|song_title|    artist|duration_seconds|\n",
            "+----------+----------+----------------+\n",
            "|   Perfect|Ed Sheeran|             250|\n",
            "+----------+----------+----------------+\n",
            "\n",
            "+----------+------------------+\n",
            "|    artist|  average_duration|\n",
            "+----------+------------------+\n",
            "|  Dua Lipa|203.33333333333334|\n",
            "|Ed Sheeran|226.66666666666666|\n",
            "|The Weeknd|             210.0|\n",
            "+----------+------------------+\n",
            "\n",
            "+-------+---------------+----------+----+\n",
            "|user_id|     song_title|play_count|rank|\n",
            "+-------+---------------+----------+----+\n",
            "|      1|Blinding Lights|         1|   1|\n",
            "|      1|        Starboy|         1|   2|\n",
            "|      1|Save Your Tears|         1|   3|\n",
            "|      2|    Galway Girl|         1|   1|\n",
            "|      2|   Shape of You|         1|   2|\n",
            "|      2|        Perfect|         1|   3|\n",
            "|      3|Don't Start Now|         1|   1|\n",
            "|      3|      New Rules|         1|   2|\n",
            "|      3|     Levitating|         1|   3|\n",
            "+-------+---------------+----------+----+\n",
            "\n",
            "+-----------+-------------+\n",
            "|stream_date|total_streams|\n",
            "+-----------+-------------+\n",
            "| 2023-09-01|            5|\n",
            "| 2023-09-02|            4|\n",
            "+-----------+-------------+\n",
            "\n",
            "+-------+------------+\n",
            "|user_id|artist_count|\n",
            "+-------+------------+\n",
            "+-------+------------+\n",
            "\n",
            "+-----------+-------------+\n",
            "|   location|total_streams|\n",
            "+-----------+-------------+\n",
            "|Los Angeles|            3|\n",
            "|     London|            3|\n",
            "|   New York|            3|\n",
            "+-----------+-------------+\n",
            "\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+-----------+\n",
            "|user_id|     song_title|    artist|duration_seconds|     streaming_time|   location|song_length|\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+-----------+\n",
            "|      1|Blinding Lights|The Weeknd|             200|2023-09-01 08:15:00|   New York|      Short|\n",
            "|      2|   Shape of You|Ed Sheeran|             240|2023-09-01 09:20:00|Los Angeles|       Long|\n",
            "|      3|     Levitating|  Dua Lipa|             180|2023-09-01 10:30:00|     London|      Short|\n",
            "|      1|        Starboy|The Weeknd|             220|2023-09-01 11:00:00|   New York|       Long|\n",
            "|      2|        Perfect|Ed Sheeran|             250|2023-09-01 12:15:00|Los Angeles|       Long|\n",
            "|      3|Don't Start Now|  Dua Lipa|             200|2023-09-02 08:10:00|     London|      Short|\n",
            "|      1|Save Your Tears|The Weeknd|             210|2023-09-02 09:00:00|   New York|       Long|\n",
            "|      2|    Galway Girl|Ed Sheeran|             190|2023-09-02 10:00:00|Los Angeles|      Short|\n",
            "|      3|      New Rules|  Dua Lipa|             230|2023-09-02 11:00:00|     London|       Long|\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "spark = SparkSession.builder.appName(\"RetailStore\").getOrCreate()\n",
        "\n",
        "retail_df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/content/sample_data/retail_data.csv\")\n",
        "\n",
        "# 1. Calculate the Total Revenue per Category\n",
        "\n",
        "total_revenue_per_category = retail_df.withColumn(\"total_revenue\", col(\"price\") * col(\"quantity\")).groupBy(\"category\").agg(F.sum(\"total_revenue\").alias(\"total_revenue\"))\n",
        "\n",
        "total_revenue_per_category.show()\n",
        "\n",
        "# 2. Filter Transactions Where the Total Sales Amount is Greater Than $100\n",
        "\n",
        "high_transactions = retail_df.withColumn(\"total_sales\", col(\"price\") * col(\"quantity\")).filter(col(\"total_sales\") > 100)\n",
        "\n",
        "high_transactions.show()\n",
        "\n",
        "# 3. Find the Most Sold Product\n",
        "\n",
        "most_sold_product = retail_df.groupBy(\"product_name\").agg(F.sum(\"quantity\").alias(\"total_quantity\")).orderBy(col(\"total_quantity\").desc()).limit(1)\n",
        "most_sold_product.show()\n",
        "\n",
        "# 4. Calculate the Average Price per Product Category\n",
        "\n",
        "avg_price_category = retail_df.groupBy(\"category\").agg(F.avg(\"price\").alias(\"average_price\"))\n",
        "avg_price_category.show()\n",
        "\n",
        "# 5. Find the Top 3 Highest Grossing Products\n",
        "\n",
        "top_grossing_products = retail_df.withColumn(\"total_revenue\", col(\"price\") * col(\"quantity\")).groupBy(\"product_name\").agg(F.sum(\"total_revenue\").alias(\"total_revenue\")) \\\n",
        ".orderBy(col(\"total_revenue\").desc()).limit(3)\n",
        "\n",
        "top_grossing_products.show()\n",
        "\n",
        "# 6. Calculate the Total Number of Items Sold per Day\n",
        "\n",
        "items_sold_perDay = retail_df.groupBy(\"sales_date\").agg(F.sum(\"quantity\").alias(\"total_quantity\"))\n",
        "items_sold_perDay.show()\n",
        "\n",
        "# 7. Identify the Product with the Lowest Price in Each Category\n",
        "\n",
        "lowest_cost = retail_df.groupBy(\"category\").agg(F.min(\"price\").alias(\"price\"))\n",
        "lowest_cost.show()\n",
        "\n",
        "# 8. Calculate the Total Revenue for Each Product\n",
        "\n",
        "revenue_product = retail_df.withColumn(\"total_revenue\", col(\"price\") * col(\"quantity\")).groupBy(\"product_name\").agg(F.sum(\"total_revenue\").alias(\"total_revenue\"))\n",
        "revenue_product.show()\n",
        "\n",
        "# 9. Find the Total Sales per Day for Each Category\n",
        "\n",
        "total_sales_per_category = retail_df.withColumn(\"total_sales\", col(\"price\") * col(\"quantity\")).groupBy(\"sales_date\", \"category\").agg(F.sum(\"total_sales\").alias(\"total_sales\"))\n",
        "\n",
        "total_sales_per_category.show()\n",
        "\n",
        "# 10. Create a New Column for Discounted Price\n",
        "\n",
        "retail_df = retail_df.withColumn(\"discounted_price\", col(\"price\") * 0.9)\n",
        "\n",
        "retail_df.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PG1Rr7gJjSsH",
        "outputId": "0e4d578d-e26a-4b62-d1df-fc26668ae878"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------------+\n",
            "|   category|     total_revenue|\n",
            "+-----------+------------------+\n",
            "| Stationery|              20.0|\n",
            "|  Groceries|13.399999999999999|\n",
            "|Electronics|            1000.0|\n",
            "|   Clothing|             155.0|\n",
            "+-----------+------------------+\n",
            "\n",
            "+--------------+------------+-----------+-----+--------+----------+-----------+\n",
            "|transaction_id|product_name|   category|price|quantity|sales_date|total_sales|\n",
            "+--------------+------------+-----------+-----+--------+----------+-----------+\n",
            "|             5|      Laptop|Electronics|800.0|       1|2023-09-03|      800.0|\n",
            "|             7|  Headphones|Electronics|100.0|       2|2023-09-04|      200.0|\n",
            "+--------------+------------+-----------+-----+--------+----------+-----------+\n",
            "\n",
            "+------------+--------------+\n",
            "|product_name|total_quantity|\n",
            "+------------+--------------+\n",
            "|      Banana|            12|\n",
            "+------------+--------------+\n",
            "\n",
            "+-----------+------------------+\n",
            "|   category|     average_price|\n",
            "+-----------+------------------+\n",
            "| Stationery|               1.5|\n",
            "|  Groceries|0.4666666666666666|\n",
            "|Electronics|             450.0|\n",
            "|   Clothing|              30.0|\n",
            "+-----------+------------------+\n",
            "\n",
            "+------------+-------------+\n",
            "|product_name|total_revenue|\n",
            "+------------+-------------+\n",
            "|      Laptop|        800.0|\n",
            "|  Headphones|        200.0|\n",
            "|       Pants|         75.0|\n",
            "+------------+-------------+\n",
            "\n",
            "+----------+--------------+\n",
            "|sales_date|total_quantity|\n",
            "+----------+--------------+\n",
            "|2023-09-03|             4|\n",
            "|2023-09-01|            12|\n",
            "|2023-09-05|             9|\n",
            "|2023-09-02|            17|\n",
            "|2023-09-04|            12|\n",
            "+----------+--------------+\n",
            "\n",
            "+-----------+-----+\n",
            "|   category|price|\n",
            "+-----------+-----+\n",
            "| Stationery|  1.0|\n",
            "|  Groceries|  0.3|\n",
            "|Electronics|100.0|\n",
            "|   Clothing| 15.0|\n",
            "+-----------+-----+\n",
            "\n",
            "+------------+------------------+\n",
            "|product_name|     total_revenue|\n",
            "+------------+------------------+\n",
            "|     T-shirt|              30.0|\n",
            "|    Sneakers|              50.0|\n",
            "|      Orange|               4.8|\n",
            "|      Banana|3.5999999999999996|\n",
            "|         Pen|              10.0|\n",
            "|       Pants|              75.0|\n",
            "|      Laptop|             800.0|\n",
            "|    Notebook|              10.0|\n",
            "|       Apple|               5.0|\n",
            "|  Headphones|             200.0|\n",
            "+------------+------------------+\n",
            "\n",
            "+----------+-----------+------------------+\n",
            "|sales_date|   category|       total_sales|\n",
            "+----------+-----------+------------------+\n",
            "|2023-09-03|Electronics|             800.0|\n",
            "|2023-09-01|  Groceries|               5.0|\n",
            "|2023-09-01|   Clothing|              30.0|\n",
            "|2023-09-02| Stationery|              10.0|\n",
            "|2023-09-04| Stationery|              10.0|\n",
            "|2023-09-02|  Groceries|3.5999999999999996|\n",
            "|2023-09-05|  Groceries|               4.8|\n",
            "|2023-09-05|   Clothing|              50.0|\n",
            "|2023-09-03|   Clothing|              75.0|\n",
            "|2023-09-04|Electronics|             200.0|\n",
            "+----------+-----------+------------------+\n",
            "\n",
            "+--------------+------------+-----------+-----+--------+----------+----------------+\n",
            "|transaction_id|product_name|   category|price|quantity|sales_date|discounted_price|\n",
            "+--------------+------------+-----------+-----+--------+----------+----------------+\n",
            "|             1|       Apple|  Groceries|  0.5|      10|2023-09-01|            0.45|\n",
            "|             2|     T-shirt|   Clothing| 15.0|       2|2023-09-01|            13.5|\n",
            "|             3|    Notebook| Stationery|  2.0|       5|2023-09-02|             1.8|\n",
            "|             4|      Banana|  Groceries|  0.3|      12|2023-09-02|            0.27|\n",
            "|             5|      Laptop|Electronics|800.0|       1|2023-09-03|           720.0|\n",
            "|             6|       Pants|   Clothing| 25.0|       3|2023-09-03|            22.5|\n",
            "|             7|  Headphones|Electronics|100.0|       2|2023-09-04|            90.0|\n",
            "|             8|         Pen| Stationery|  1.0|      10|2023-09-04|             0.9|\n",
            "|             9|      Orange|  Groceries|  0.6|       8|2023-09-05|            0.54|\n",
            "|            10|    Sneakers|   Clothing| 50.0|       1|2023-09-05|            45.0|\n",
            "+--------------+------------+-----------+-----+--------+----------+----------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}