{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48c5cc7d-9d90-4673-93fa-e8e29d0566c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    \"OrderID\": [1, 2, 3, 4],\n",
    "    \"OrderDate\": [\"2024-01-01 10:00:00\", \"2024-01-02 11:00:00\", \"2024-01-03 12:00:00\", \"2024-01-04 13:00:00\"],\n",
    "    \"CustomerID\": [\"C001\", \"C002\", \"C003\", \"C004\"],\n",
    "    \"Product\": [\"ProductA\", \"ProductB\", \"ProductC\", \"ProductD\"],\n",
    "    \"Quantity\": [10, 20, 15, 5],\n",
    "    \"Price\": [100.0, 200.0, 150.0, 50.0]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to a local path first\n",
    "local_csv_path = \"/tmp/sales2_data.csv\"\n",
    "local_parquet_path = \"/tmp/sales_data.parquet\"\n",
    "\n",
    "# Save to CSV and Parquet locally\n",
    "df.to_csv(local_csv_path, index=False)\n",
    "df.to_parquet(local_parquet_path, index=False)\n",
    "\n",
    "# Now, move the files to DBFS\n",
    "dbutils.fs.mv(f\"file:{local_csv_path}\", \"dbfs:/tmp/sales2_data.csv\")\n",
    "dbutils.fs.mv(f\"file:{local_parquet_path}\", \"dbfs:/tmp/sales_data.parquet\")\n",
    "\n",
    "print(\"Files successfully moved to DBFS.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62196291-bef5-4362-b853-70943199729f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta table created\n"
     ]
    }
   ],
   "source": [
    "# Initialize SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DelatExample\").getOrCreate()\n",
    "\n",
    "# Load data from CSV\n",
    "df_sales = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"dbfs:/tmp/sales2_data.csv\")\n",
    "\n",
    "# Transform the data\n",
    "df_transformed = df_sales.withColumn(\"TotalAmount\", col(\"Quantity\").cast(\"int\") * col(\"Price\").cast(\"double\"))\n",
    "\n",
    "# Write transformed data to delta table\n",
    "delta_table_path = '/delta/sales_data'\n",
    "df_transformed.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "\n",
    "print(\"Delta table created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54a14b68-9041-44fc-9403-d35d0591ea5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1461438442376509>, line 3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdlt\u001B[39;00m\n",
       "\u001B[0;32m----> 3\u001B[0m \u001B[38;5;129m@dlt\u001B[39m\u001B[38;5;241m.\u001B[39mtable\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msales_data\u001B[39m():\n",
       "\u001B[1;32m      5\u001B[0m     df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mload(delta_table_path)\n",
       "\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\u001B[38;5;241m.\u001B[39mselect(\n",
       "\u001B[1;32m      7\u001B[0m         col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOrderID\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[1;32m      8\u001B[0m         col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOrderDate\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     13\u001B[0m         (col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mQuantity\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mint\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m*\u001B[39m col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPrice\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdouble\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTotalAmount\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     14\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/dlt/api.py:431\u001B[0m, in \u001B[0;36mtable\u001B[0;34m(query_function, name, comment, spark_conf, table_properties, partition_cols, path, schema, temporary, cluster_by, row_filter)\u001B[0m\n",
       "\u001B[1;32m    429\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m query_function \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    430\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(query_function):\n",
       "\u001B[0;32m--> 431\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m outer(query_function)\n",
       "\u001B[1;32m    432\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    433\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n",
       "\u001B[1;32m    434\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe first positional argument passed to @table must be callable. Either add @table\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    435\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m with no parameters to your query function, or pass options to @table using\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    436\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m keyword arguments (e.g. @table(name=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtable_a\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m)).\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/dlt/api.py:401\u001B[0m, in \u001B[0;36mtable.<locals>.outer\u001B[0;34m(func)\u001B[0m\n",
       "\u001B[1;32m    400\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mouter\u001B[39m(func):\n",
       "\u001B[0;32m--> 401\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m _register_table(func\u001B[38;5;241m=\u001B[39mfunc,\n",
       "\u001B[1;32m    402\u001B[0m                               name\u001B[38;5;241m=\u001B[39mname,\n",
       "\u001B[1;32m    403\u001B[0m                               comment\u001B[38;5;241m=\u001B[39mcomment,\n",
       "\u001B[1;32m    404\u001B[0m                               spark_conf\u001B[38;5;241m=\u001B[39mspark_conf,\n",
       "\u001B[1;32m    405\u001B[0m                               table_properties\u001B[38;5;241m=\u001B[39mtable_properties,\n",
       "\u001B[1;32m    406\u001B[0m                               partition_cols\u001B[38;5;241m=\u001B[39mpartition_cols,\n",
       "\u001B[1;32m    407\u001B[0m                               path\u001B[38;5;241m=\u001B[39mpath,\n",
       "\u001B[1;32m    408\u001B[0m                               schema\u001B[38;5;241m=\u001B[39mschema,\n",
       "\u001B[1;32m    409\u001B[0m                               temporary\u001B[38;5;241m=\u001B[39mtemporary,\n",
       "\u001B[1;32m    410\u001B[0m                               cluster_by\u001B[38;5;241m=\u001B[39mcluster_by,\n",
       "\u001B[1;32m    411\u001B[0m                               row_filter\u001B[38;5;241m=\u001B[39mrow_filter)\n",
       "\u001B[1;32m    413\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_dlt_get_dataset\u001B[39m():\n",
       "\u001B[1;32m    414\u001B[0m         \u001B[38;5;66;03m# If decorators are chained together, this is the function that will be passed to the\u001B[39;00m\n",
       "\u001B[1;32m    415\u001B[0m         \u001B[38;5;66;03m# next decorator.\u001B[39;00m\n",
       "\u001B[1;32m    416\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m dataset\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/dlt/api.py:267\u001B[0m, in \u001B[0;36m_register_table\u001B[0;34m(func, name, comment, spark_conf, table_properties, partition_cols, path, schema, temporary, cluster_by, row_filter)\u001B[0m\n",
       "\u001B[1;32m    252\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_register_table\u001B[39m(\n",
       "\u001B[1;32m    253\u001B[0m         func,\n",
       "\u001B[1;32m    254\u001B[0m         name,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    262\u001B[0m         cluster_by,\n",
       "\u001B[1;32m    263\u001B[0m         row_filter):\n",
       "\u001B[1;32m    264\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    265\u001B[0m \u001B[38;5;124;03m    Registers the table in the underlying Scala pipeline with all the specified table properties.\u001B[39;00m\n",
       "\u001B[1;32m    266\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m--> 267\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m _register_dataset(\n",
       "\u001B[1;32m    268\u001B[0m         func\u001B[38;5;241m=\u001B[39mfunc,\n",
       "\u001B[1;32m    269\u001B[0m         is_table\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n",
       "\u001B[1;32m    270\u001B[0m         name\u001B[38;5;241m=\u001B[39mname,\n",
       "\u001B[1;32m    271\u001B[0m         comment\u001B[38;5;241m=\u001B[39mcomment,\n",
       "\u001B[1;32m    272\u001B[0m         spark_conf\u001B[38;5;241m=\u001B[39mspark_conf)\n",
       "\u001B[1;32m    273\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m temporary \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n",
       "\u001B[1;32m    274\u001B[0m         dataset\u001B[38;5;241m.\u001B[39mbuilder \u001B[38;5;241m=\u001B[39m dataset\u001B[38;5;241m.\u001B[39mbuilder\u001B[38;5;241m.\u001B[39mtemporary()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/dlt/api.py:130\u001B[0m, in \u001B[0;36m_register_dataset\u001B[0;34m(func, is_table, name, comment, spark_conf)\u001B[0m\n",
       "\u001B[1;32m    126\u001B[0m pipeline\u001B[38;5;241m.\u001B[39m_add_graph_entity(dataset\u001B[38;5;241m.\u001B[39mname, pipeline\u001B[38;5;241m.\u001B[39m_GraphEntityType\u001B[38;5;241m.\u001B[39mDATASET)\n",
       "\u001B[1;32m    128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_table:\n",
       "\u001B[1;32m    129\u001B[0m     dataset\u001B[38;5;241m.\u001B[39mbuilder \u001B[38;5;241m=\u001B[39m (\n",
       "\u001B[0;32m--> 130\u001B[0m         pipeline\u001B[38;5;241m.\u001B[39minstance\u001B[38;5;241m.\u001B[39mget_scala_pipeline()\u001B[38;5;241m.\u001B[39mcreateTable(name)\n",
       "\u001B[1;32m    131\u001B[0m         \u001B[38;5;241m.\u001B[39mquery(FlowFunction(dataset\u001B[38;5;241m.\u001B[39mfunc))\n",
       "\u001B[1;32m    132\u001B[0m     )\n",
       "\u001B[1;32m    133\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    134\u001B[0m     dataset\u001B[38;5;241m.\u001B[39mbuilder \u001B[38;5;241m=\u001B[39m (\n",
       "\u001B[1;32m    135\u001B[0m         pipeline\u001B[38;5;241m.\u001B[39minstance\u001B[38;5;241m.\u001B[39mget_scala_pipeline()\u001B[38;5;241m.\u001B[39mcreateView(name)\n",
       "\u001B[1;32m    136\u001B[0m         \u001B[38;5;241m.\u001B[39mquery(FlowFunction(dataset\u001B[38;5;241m.\u001B[39mfunc))\n",
       "\u001B[1;32m    137\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n",
       "\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:261\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    257\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    259\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    260\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 261\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    263\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Cannot redefine dataset `sales_data`"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "Cannot redefine dataset `sales_data`"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Cannot redefine dataset `sales_data`"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": null,
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "sqlState": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-1461438442376509>, line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdlt\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;129m@dlt\u001B[39m\u001B[38;5;241m.\u001B[39mtable\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msales_data\u001B[39m():\n\u001B[1;32m      5\u001B[0m     df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mload(delta_table_path)\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\u001B[38;5;241m.\u001B[39mselect(\n\u001B[1;32m      7\u001B[0m         col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOrderID\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m      8\u001B[0m         col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOrderDate\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     13\u001B[0m         (col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mQuantity\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mint\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m*\u001B[39m col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPrice\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdouble\u001B[39m\u001B[38;5;124m\"\u001B[39m))\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTotalAmount\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     14\u001B[0m     )\n",
        "File \u001B[0;32m/databricks/spark/python/dlt/api.py:431\u001B[0m, in \u001B[0;36mtable\u001B[0;34m(query_function, name, comment, spark_conf, table_properties, partition_cols, path, schema, temporary, cluster_by, row_filter)\u001B[0m\n\u001B[1;32m    429\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m query_function \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    430\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(query_function):\n\u001B[0;32m--> 431\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m outer(query_function)\n\u001B[1;32m    432\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    433\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    434\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe first positional argument passed to @table must be callable. Either add @table\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    435\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m with no parameters to your query function, or pass options to @table using\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    436\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m keyword arguments (e.g. @table(name=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtable_a\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m)).\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/databricks/spark/python/dlt/api.py:401\u001B[0m, in \u001B[0;36mtable.<locals>.outer\u001B[0;34m(func)\u001B[0m\n\u001B[1;32m    400\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mouter\u001B[39m(func):\n\u001B[0;32m--> 401\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m _register_table(func\u001B[38;5;241m=\u001B[39mfunc,\n\u001B[1;32m    402\u001B[0m                               name\u001B[38;5;241m=\u001B[39mname,\n\u001B[1;32m    403\u001B[0m                               comment\u001B[38;5;241m=\u001B[39mcomment,\n\u001B[1;32m    404\u001B[0m                               spark_conf\u001B[38;5;241m=\u001B[39mspark_conf,\n\u001B[1;32m    405\u001B[0m                               table_properties\u001B[38;5;241m=\u001B[39mtable_properties,\n\u001B[1;32m    406\u001B[0m                               partition_cols\u001B[38;5;241m=\u001B[39mpartition_cols,\n\u001B[1;32m    407\u001B[0m                               path\u001B[38;5;241m=\u001B[39mpath,\n\u001B[1;32m    408\u001B[0m                               schema\u001B[38;5;241m=\u001B[39mschema,\n\u001B[1;32m    409\u001B[0m                               temporary\u001B[38;5;241m=\u001B[39mtemporary,\n\u001B[1;32m    410\u001B[0m                               cluster_by\u001B[38;5;241m=\u001B[39mcluster_by,\n\u001B[1;32m    411\u001B[0m                               row_filter\u001B[38;5;241m=\u001B[39mrow_filter)\n\u001B[1;32m    413\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_dlt_get_dataset\u001B[39m():\n\u001B[1;32m    414\u001B[0m         \u001B[38;5;66;03m# If decorators are chained together, this is the function that will be passed to the\u001B[39;00m\n\u001B[1;32m    415\u001B[0m         \u001B[38;5;66;03m# next decorator.\u001B[39;00m\n\u001B[1;32m    416\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m dataset\n",
        "File \u001B[0;32m/databricks/spark/python/dlt/api.py:267\u001B[0m, in \u001B[0;36m_register_table\u001B[0;34m(func, name, comment, spark_conf, table_properties, partition_cols, path, schema, temporary, cluster_by, row_filter)\u001B[0m\n\u001B[1;32m    252\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_register_table\u001B[39m(\n\u001B[1;32m    253\u001B[0m         func,\n\u001B[1;32m    254\u001B[0m         name,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    262\u001B[0m         cluster_by,\n\u001B[1;32m    263\u001B[0m         row_filter):\n\u001B[1;32m    264\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;124;03m    Registers the table in the underlying Scala pipeline with all the specified table properties.\u001B[39;00m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 267\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m _register_dataset(\n\u001B[1;32m    268\u001B[0m         func\u001B[38;5;241m=\u001B[39mfunc,\n\u001B[1;32m    269\u001B[0m         is_table\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    270\u001B[0m         name\u001B[38;5;241m=\u001B[39mname,\n\u001B[1;32m    271\u001B[0m         comment\u001B[38;5;241m=\u001B[39mcomment,\n\u001B[1;32m    272\u001B[0m         spark_conf\u001B[38;5;241m=\u001B[39mspark_conf)\n\u001B[1;32m    273\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m temporary \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    274\u001B[0m         dataset\u001B[38;5;241m.\u001B[39mbuilder \u001B[38;5;241m=\u001B[39m dataset\u001B[38;5;241m.\u001B[39mbuilder\u001B[38;5;241m.\u001B[39mtemporary()\n",
        "File \u001B[0;32m/databricks/spark/python/dlt/api.py:130\u001B[0m, in \u001B[0;36m_register_dataset\u001B[0;34m(func, is_table, name, comment, spark_conf)\u001B[0m\n\u001B[1;32m    126\u001B[0m pipeline\u001B[38;5;241m.\u001B[39m_add_graph_entity(dataset\u001B[38;5;241m.\u001B[39mname, pipeline\u001B[38;5;241m.\u001B[39m_GraphEntityType\u001B[38;5;241m.\u001B[39mDATASET)\n\u001B[1;32m    128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_table:\n\u001B[1;32m    129\u001B[0m     dataset\u001B[38;5;241m.\u001B[39mbuilder \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 130\u001B[0m         pipeline\u001B[38;5;241m.\u001B[39minstance\u001B[38;5;241m.\u001B[39mget_scala_pipeline()\u001B[38;5;241m.\u001B[39mcreateTable(name)\n\u001B[1;32m    131\u001B[0m         \u001B[38;5;241m.\u001B[39mquery(FlowFunction(dataset\u001B[38;5;241m.\u001B[39mfunc))\n\u001B[1;32m    132\u001B[0m     )\n\u001B[1;32m    133\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    134\u001B[0m     dataset\u001B[38;5;241m.\u001B[39mbuilder \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    135\u001B[0m         pipeline\u001B[38;5;241m.\u001B[39minstance\u001B[38;5;241m.\u001B[39mget_scala_pipeline()\u001B[38;5;241m.\u001B[39mcreateView(name)\n\u001B[1;32m    136\u001B[0m         \u001B[38;5;241m.\u001B[39mquery(FlowFunction(dataset\u001B[38;5;241m.\u001B[39mfunc))\n\u001B[1;32m    137\u001B[0m     )\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:261\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    257\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    259\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    260\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 261\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    263\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
        "\u001B[0;31mAnalysisException\u001B[0m: Cannot redefine dataset `sales_data`"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dlt\n",
    "\n",
    "@dlt.table\n",
    "def sales_data():\n",
    "    df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "    return df.select(\n",
    "        col(\"OrderID\"),\n",
    "        col(\"OrderDate\"),\n",
    "        col(\"CustomerID\"),\n",
    "        col(\"Product\"),\n",
    "        col(\"Quantity\"),\n",
    "        col(\"Price\"),\n",
    "        (col(\"Quantity\").cast(\"int\") * col(\"Price\").cast(\"double\")).alias(\"TotalAmount\")\n",
    "    )\n",
    "\n",
    "print(\"Delta live table created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc638a4c-aba6-4f9a-a943-5b73052c6dd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Delta table created and data written successfully.\nNew data appended to Delta table successfully.\nMerging new data into Delta table...\nData merged successfully.\nViewing Delta table history...\n+-------+-------------------+----------------+---------------------+---------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n|version|timestamp          |userId          |userName             |operation|operationParameters                                                                                                                                                                                  |job |notebook          |clusterId           |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |userMetadata|engineInfo                         |\n+-------+-------------------+----------------+---------------------+---------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n|5      |2024-09-16 12:07:35|8407505950383760|chandra2sai@gmail.com|MERGE    |{predicate -> [\"(ID#4140L = ID#3674L)\"], matchedPredicates -> [{\"actionType\":\"update\"}], statsOnLoad -> false, notMatchedBySourcePredicates -> [], notMatchedPredicates -> [{\"actionType\":\"insert\"}]}|NULL|{1461438442376506}|0916-114954-oocivncp|4          |WriteSerializable|false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 2, numTargetBytesAdded -> 1561, numTargetBytesRemoved -> 2240, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 3, executionTimeMs -> 8989, materializeSourceTimeMs -> 368, numTargetRowsInserted -> 0, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 4248, numTargetRowsUpdated -> 3, numOutputRows -> 3, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 2, numTargetFilesRemoved -> 3, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 4121}   |NULL        |Databricks-Runtime/15.4.x-scala2.12|\n|4      |2024-09-16 12:07:22|8407505950383760|chandra2sai@gmail.com|WRITE    |{mode -> Append, statsOnLoad -> false, partitionBy -> []}                                                                                                                                            |NULL|{1461438442376506}|0916-114954-oocivncp|3          |WriteSerializable|true         |{numFiles -> 2, numOutputRows -> 2, numOutputBytes -> 1493}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |NULL        |Databricks-Runtime/15.4.x-scala2.12|\n|3      |2024-09-16 12:07:15|8407505950383760|chandra2sai@gmail.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                                         |NULL|{1461438442376506}|0916-114954-oocivncp|2          |WriteSerializable|false        |{numFiles -> 3, numOutputRows -> 3, numOutputBytes -> 2241}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |NULL        |Databricks-Runtime/15.4.x-scala2.12|\n|2      |2024-09-16 12:05:56|8407505950383760|chandra2sai@gmail.com|MERGE    |{predicate -> [\"(ID#1104L = ID#638L)\"], matchedPredicates -> [{\"actionType\":\"update\"}], statsOnLoad -> false, notMatchedBySourcePredicates -> [], notMatchedPredicates -> [{\"actionType\":\"insert\"}]} |NULL|{1461438442376506}|0916-114954-oocivncp|1          |WriteSerializable|false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 2, numTargetBytesAdded -> 1561, numTargetBytesRemoved -> 2240, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 3, executionTimeMs -> 21821, materializeSourceTimeMs -> 1205, numTargetRowsInserted -> 0, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 14892, numTargetRowsUpdated -> 3, numOutputRows -> 3, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 2, numTargetFilesRemoved -> 3, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 5274}|NULL        |Databricks-Runtime/15.4.x-scala2.12|\n|1      |2024-09-16 12:05:27|8407505950383760|chandra2sai@gmail.com|WRITE    |{mode -> Append, statsOnLoad -> false, partitionBy -> []}                                                                                                                                            |NULL|{1461438442376506}|0916-114954-oocivncp|0          |WriteSerializable|true         |{numFiles -> 2, numOutputRows -> 2, numOutputBytes -> 1493}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |NULL        |Databricks-Runtime/15.4.x-scala2.12|\n|0      |2024-09-16 12:05:06|8407505950383760|chandra2sai@gmail.com|WRITE    |{mode -> Overwrite, statsOnLoad -> false, partitionBy -> []}                                                                                                                                         |NULL|{1461438442376506}|0916-114954-oocivncp|NULL       |WriteSerializable|false        |{numFiles -> 3, numOutputRows -> 3, numOutputBytes -> 2241}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |NULL        |Databricks-Runtime/15.4.x-scala2.12|\n+-------+-------------------+----------------+---------------------+---------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n\nQuerying Delta table as of version 0...\n+---+-----+\n|ID |Value|\n+---+-----+\n|1  |100  |\n|2  |200  |\n|3  |300  |\n+---+-----+\n\nVacuuming old files...\nDelta operations completed.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaOperationsSimpleExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Delta table path\n",
    "delta_table_path = \"/delta/simple_data\"\n",
    "\n",
    "# Define initial sample data\n",
    "initial_data = [\n",
    "    (1, 100),\n",
    "    (2, 200),\n",
    "    (3, 300)\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "schema = [\"ID\", \"Value\"]\n",
    "\n",
    "# Create DataFrame for initial data\n",
    "df_initial = spark.createDataFrame(initial_data, schema=schema)\n",
    "\n",
    "# Write DataFrame to Delta table\n",
    "df_initial.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "\n",
    "print(\"Initial Delta table created and data written successfully.\")\n",
    "\n",
    "# Define new sample data\n",
    "new_sample_data = [\n",
    "    (2, 250),  # Existing ID with updated Value\n",
    "    (4, 400)   # New ID\n",
    "]\n",
    "\n",
    "# Create DataFrame for new data\n",
    "df_new = spark.createDataFrame(new_sample_data, schema=schema)\n",
    "\n",
    "# Write the new data to Delta table in append mode\n",
    "df_new.write.format(\"delta\").mode(\"append\").save(delta_table_path)\n",
    "\n",
    "print(\"New data appended to Delta table successfully.\")\n",
    "\n",
    "# Create a temporary view for SQL operations\n",
    "df_new.createOrReplaceTempView(\"new_data\")\n",
    "\n",
    "# Perform the merge operation\n",
    "print(\"Merging new data into Delta table...\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "MERGE INTO delta.`{delta_table_path}` AS target\n",
    "USING new_data AS source\n",
    "ON target.ID = source.ID\n",
    "WHEN MATCHED THEN UPDATE SET\n",
    "    target.Value = source.Value\n",
    "WHEN NOT MATCHED THEN INSERT (\n",
    "    ID,\n",
    "    Value\n",
    ") VALUES (\n",
    "    source.ID,\n",
    "    source.Value\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "print(\"Data merged successfully.\")\n",
    "\n",
    "# Delta operations - History, Time Travel, and Vacuum\n",
    "print(\"Viewing Delta table history...\")\n",
    "history_df = spark.sql(f\"DESCRIBE HISTORY delta.`{delta_table_path}`\")\n",
    "history_df.show(truncate=False)\n",
    "\n",
    "print(\"Querying Delta table as of version 0...\")\n",
    "df_time_travel = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n",
    "df_time_travel.show(truncate=False)\n",
    "\n",
    "print(\"Vacuuming old files...\")\n",
    "spark.sql(f\"VACUUM delta.`{delta_table_path}` RETAIN 168 HOURS\")\n",
    "\n",
    "print(\"Delta operations completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Delta_live_table_processing",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
