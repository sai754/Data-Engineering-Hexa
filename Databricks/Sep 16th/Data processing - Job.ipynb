{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1410f99-7e95-4de2-a155-9d1be6258723",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3172890228810657>, line 16\u001B[0m\n",
       "\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# Save to csv\u001B[39;00m\n",
       "\u001B[1;32m     15\u001B[0m csv_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/dbfs/FileStore/sales2_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m---> 16\u001B[0m df\u001B[38;5;241m.\u001B[39mto_csv(csv_path,index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
       "\u001B[1;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSample data save to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcsv_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pandas/util/_decorators.py:211\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    209\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    210\u001B[0m         kwargs[new_arg_name] \u001B[38;5;241m=\u001B[39m new_arg_value\n",
       "\u001B[0;32m--> 211\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pandas/core/generic.py:3720\u001B[0m, in \u001B[0;36mNDFrame.to_csv\u001B[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001B[0m\n",
       "\u001B[1;32m   3709\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m, ABCDataFrame) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mto_frame()\n",
       "\u001B[1;32m   3711\u001B[0m formatter \u001B[38;5;241m=\u001B[39m DataFrameFormatter(\n",
       "\u001B[1;32m   3712\u001B[0m     frame\u001B[38;5;241m=\u001B[39mdf,\n",
       "\u001B[1;32m   3713\u001B[0m     header\u001B[38;5;241m=\u001B[39mheader,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3717\u001B[0m     decimal\u001B[38;5;241m=\u001B[39mdecimal,\n",
       "\u001B[1;32m   3718\u001B[0m )\n",
       "\u001B[0;32m-> 3720\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrameRenderer(formatter)\u001B[38;5;241m.\u001B[39mto_csv(\n",
       "\u001B[1;32m   3721\u001B[0m     path_or_buf,\n",
       "\u001B[1;32m   3722\u001B[0m     lineterminator\u001B[38;5;241m=\u001B[39mlineterminator,\n",
       "\u001B[1;32m   3723\u001B[0m     sep\u001B[38;5;241m=\u001B[39msep,\n",
       "\u001B[1;32m   3724\u001B[0m     encoding\u001B[38;5;241m=\u001B[39mencoding,\n",
       "\u001B[1;32m   3725\u001B[0m     errors\u001B[38;5;241m=\u001B[39merrors,\n",
       "\u001B[1;32m   3726\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n",
       "\u001B[1;32m   3727\u001B[0m     quoting\u001B[38;5;241m=\u001B[39mquoting,\n",
       "\u001B[1;32m   3728\u001B[0m     columns\u001B[38;5;241m=\u001B[39mcolumns,\n",
       "\u001B[1;32m   3729\u001B[0m     index_label\u001B[38;5;241m=\u001B[39mindex_label,\n",
       "\u001B[1;32m   3730\u001B[0m     mode\u001B[38;5;241m=\u001B[39mmode,\n",
       "\u001B[1;32m   3731\u001B[0m     chunksize\u001B[38;5;241m=\u001B[39mchunksize,\n",
       "\u001B[1;32m   3732\u001B[0m     quotechar\u001B[38;5;241m=\u001B[39mquotechar,\n",
       "\u001B[1;32m   3733\u001B[0m     date_format\u001B[38;5;241m=\u001B[39mdate_format,\n",
       "\u001B[1;32m   3734\u001B[0m     doublequote\u001B[38;5;241m=\u001B[39mdoublequote,\n",
       "\u001B[1;32m   3735\u001B[0m     escapechar\u001B[38;5;241m=\u001B[39mescapechar,\n",
       "\u001B[1;32m   3736\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n",
       "\u001B[1;32m   3737\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pandas/util/_decorators.py:211\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    209\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    210\u001B[0m         kwargs[new_arg_name] \u001B[38;5;241m=\u001B[39m new_arg_value\n",
       "\u001B[0;32m--> 211\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pandas/io/formats/format.py:1189\u001B[0m, in \u001B[0;36mDataFrameRenderer.to_csv\u001B[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001B[0m\n",
       "\u001B[1;32m   1168\u001B[0m     created_buffer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\u001B[1;32m   1170\u001B[0m csv_formatter \u001B[38;5;241m=\u001B[39m CSVFormatter(\n",
       "\u001B[1;32m   1171\u001B[0m     path_or_buf\u001B[38;5;241m=\u001B[39mpath_or_buf,\n",
       "\u001B[1;32m   1172\u001B[0m     lineterminator\u001B[38;5;241m=\u001B[39mlineterminator,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   1187\u001B[0m     formatter\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfmt,\n",
       "\u001B[1;32m   1188\u001B[0m )\n",
       "\u001B[0;32m-> 1189\u001B[0m csv_formatter\u001B[38;5;241m.\u001B[39msave()\n",
       "\u001B[1;32m   1191\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m created_buffer:\n",
       "\u001B[1;32m   1192\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path_or_buf, StringIO)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pandas/io/formats/csvs.py:241\u001B[0m, in \u001B[0;36mCSVFormatter.save\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    237\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    238\u001B[0m \u001B[38;5;124;03mCreate the writer & save.\u001B[39;00m\n",
       "\u001B[1;32m    239\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    240\u001B[0m \u001B[38;5;66;03m# apply compression and byte/text conversion\u001B[39;00m\n",
       "\u001B[0;32m--> 241\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m get_handle(\n",
       "\u001B[1;32m    242\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfilepath_or_buffer,\n",
       "\u001B[1;32m    243\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode,\n",
       "\u001B[1;32m    244\u001B[0m     encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoding,\n",
       "\u001B[1;32m    245\u001B[0m     errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39merrors,\n",
       "\u001B[1;32m    246\u001B[0m     compression\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompression,\n",
       "\u001B[1;32m    247\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstorage_options,\n",
       "\u001B[1;32m    248\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m handles:\n",
       "\u001B[1;32m    249\u001B[0m \n",
       "\u001B[1;32m    250\u001B[0m     \u001B[38;5;66;03m# Note: self.encoding is irrelevant here\u001B[39;00m\n",
       "\u001B[1;32m    251\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwriter \u001B[38;5;241m=\u001B[39m csvlib\u001B[38;5;241m.\u001B[39mwriter(\n",
       "\u001B[1;32m    252\u001B[0m         handles\u001B[38;5;241m.\u001B[39mhandle,\n",
       "\u001B[1;32m    253\u001B[0m         lineterminator\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlineterminator,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    258\u001B[0m         quotechar\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mquotechar,\n",
       "\u001B[1;32m    259\u001B[0m     )\n",
       "\u001B[1;32m    261\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_save()\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pandas/io/common.py:734\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n",
       "\u001B[1;32m    732\u001B[0m \u001B[38;5;66;03m# Only for write methods\u001B[39;00m\n",
       "\u001B[1;32m    733\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode \u001B[38;5;129;01mand\u001B[39;00m is_path:\n",
       "\u001B[0;32m--> 734\u001B[0m     check_parent_directory(\u001B[38;5;28mstr\u001B[39m(handle))\n",
       "\u001B[1;32m    736\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m compression:\n",
       "\u001B[1;32m    737\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m compression \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mzstd\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m    738\u001B[0m         \u001B[38;5;66;03m# compression libraries do not like an explicit text-mode\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pandas/io/common.py:597\u001B[0m, in \u001B[0;36mcheck_parent_directory\u001B[0;34m(path)\u001B[0m\n",
       "\u001B[1;32m    595\u001B[0m parent \u001B[38;5;241m=\u001B[39m Path(path)\u001B[38;5;241m.\u001B[39mparent\n",
       "\u001B[1;32m    596\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m parent\u001B[38;5;241m.\u001B[39mis_dir():\n",
       "\u001B[0;32m--> 597\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\u001B[38;5;124mrf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot save file into a non-existent directory: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparent\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "\u001B[0;31mOSError\u001B[0m: Cannot save file into a non-existent directory: '/dbfs/FileStore'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "OSError",
        "evalue": "Cannot save file into a non-existent directory: '/dbfs/FileStore'"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>OSError</span>: Cannot save file into a non-existent directory: '/dbfs/FileStore'"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
        "File \u001B[0;32m<command-3172890228810657>, line 16\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# Save to csv\u001B[39;00m\n\u001B[1;32m     15\u001B[0m csv_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/dbfs/FileStore/sales2_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 16\u001B[0m df\u001B[38;5;241m.\u001B[39mto_csv(csv_path,index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSample data save to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcsv_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pandas/util/_decorators.py:211\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    209\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    210\u001B[0m         kwargs[new_arg_name] \u001B[38;5;241m=\u001B[39m new_arg_value\n\u001B[0;32m--> 211\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pandas/core/generic.py:3720\u001B[0m, in \u001B[0;36mNDFrame.to_csv\u001B[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001B[0m\n\u001B[1;32m   3709\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m, ABCDataFrame) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mto_frame()\n\u001B[1;32m   3711\u001B[0m formatter \u001B[38;5;241m=\u001B[39m DataFrameFormatter(\n\u001B[1;32m   3712\u001B[0m     frame\u001B[38;5;241m=\u001B[39mdf,\n\u001B[1;32m   3713\u001B[0m     header\u001B[38;5;241m=\u001B[39mheader,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3717\u001B[0m     decimal\u001B[38;5;241m=\u001B[39mdecimal,\n\u001B[1;32m   3718\u001B[0m )\n\u001B[0;32m-> 3720\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrameRenderer(formatter)\u001B[38;5;241m.\u001B[39mto_csv(\n\u001B[1;32m   3721\u001B[0m     path_or_buf,\n\u001B[1;32m   3722\u001B[0m     lineterminator\u001B[38;5;241m=\u001B[39mlineterminator,\n\u001B[1;32m   3723\u001B[0m     sep\u001B[38;5;241m=\u001B[39msep,\n\u001B[1;32m   3724\u001B[0m     encoding\u001B[38;5;241m=\u001B[39mencoding,\n\u001B[1;32m   3725\u001B[0m     errors\u001B[38;5;241m=\u001B[39merrors,\n\u001B[1;32m   3726\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n\u001B[1;32m   3727\u001B[0m     quoting\u001B[38;5;241m=\u001B[39mquoting,\n\u001B[1;32m   3728\u001B[0m     columns\u001B[38;5;241m=\u001B[39mcolumns,\n\u001B[1;32m   3729\u001B[0m     index_label\u001B[38;5;241m=\u001B[39mindex_label,\n\u001B[1;32m   3730\u001B[0m     mode\u001B[38;5;241m=\u001B[39mmode,\n\u001B[1;32m   3731\u001B[0m     chunksize\u001B[38;5;241m=\u001B[39mchunksize,\n\u001B[1;32m   3732\u001B[0m     quotechar\u001B[38;5;241m=\u001B[39mquotechar,\n\u001B[1;32m   3733\u001B[0m     date_format\u001B[38;5;241m=\u001B[39mdate_format,\n\u001B[1;32m   3734\u001B[0m     doublequote\u001B[38;5;241m=\u001B[39mdoublequote,\n\u001B[1;32m   3735\u001B[0m     escapechar\u001B[38;5;241m=\u001B[39mescapechar,\n\u001B[1;32m   3736\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[1;32m   3737\u001B[0m )\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pandas/util/_decorators.py:211\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    209\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    210\u001B[0m         kwargs[new_arg_name] \u001B[38;5;241m=\u001B[39m new_arg_value\n\u001B[0;32m--> 211\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pandas/io/formats/format.py:1189\u001B[0m, in \u001B[0;36mDataFrameRenderer.to_csv\u001B[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001B[0m\n\u001B[1;32m   1168\u001B[0m     created_buffer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m   1170\u001B[0m csv_formatter \u001B[38;5;241m=\u001B[39m CSVFormatter(\n\u001B[1;32m   1171\u001B[0m     path_or_buf\u001B[38;5;241m=\u001B[39mpath_or_buf,\n\u001B[1;32m   1172\u001B[0m     lineterminator\u001B[38;5;241m=\u001B[39mlineterminator,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1187\u001B[0m     formatter\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfmt,\n\u001B[1;32m   1188\u001B[0m )\n\u001B[0;32m-> 1189\u001B[0m csv_formatter\u001B[38;5;241m.\u001B[39msave()\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m created_buffer:\n\u001B[1;32m   1192\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path_or_buf, StringIO)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pandas/io/formats/csvs.py:241\u001B[0m, in \u001B[0;36mCSVFormatter.save\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    237\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    238\u001B[0m \u001B[38;5;124;03mCreate the writer & save.\u001B[39;00m\n\u001B[1;32m    239\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    240\u001B[0m \u001B[38;5;66;03m# apply compression and byte/text conversion\u001B[39;00m\n\u001B[0;32m--> 241\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m get_handle(\n\u001B[1;32m    242\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfilepath_or_buffer,\n\u001B[1;32m    243\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode,\n\u001B[1;32m    244\u001B[0m     encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoding,\n\u001B[1;32m    245\u001B[0m     errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39merrors,\n\u001B[1;32m    246\u001B[0m     compression\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompression,\n\u001B[1;32m    247\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstorage_options,\n\u001B[1;32m    248\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m handles:\n\u001B[1;32m    249\u001B[0m \n\u001B[1;32m    250\u001B[0m     \u001B[38;5;66;03m# Note: self.encoding is irrelevant here\u001B[39;00m\n\u001B[1;32m    251\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwriter \u001B[38;5;241m=\u001B[39m csvlib\u001B[38;5;241m.\u001B[39mwriter(\n\u001B[1;32m    252\u001B[0m         handles\u001B[38;5;241m.\u001B[39mhandle,\n\u001B[1;32m    253\u001B[0m         lineterminator\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlineterminator,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    258\u001B[0m         quotechar\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mquotechar,\n\u001B[1;32m    259\u001B[0m     )\n\u001B[1;32m    261\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_save()\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pandas/io/common.py:734\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    732\u001B[0m \u001B[38;5;66;03m# Only for write methods\u001B[39;00m\n\u001B[1;32m    733\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode \u001B[38;5;129;01mand\u001B[39;00m is_path:\n\u001B[0;32m--> 734\u001B[0m     check_parent_directory(\u001B[38;5;28mstr\u001B[39m(handle))\n\u001B[1;32m    736\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m compression:\n\u001B[1;32m    737\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m compression \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mzstd\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    738\u001B[0m         \u001B[38;5;66;03m# compression libraries do not like an explicit text-mode\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pandas/io/common.py:597\u001B[0m, in \u001B[0;36mcheck_parent_directory\u001B[0;34m(path)\u001B[0m\n\u001B[1;32m    595\u001B[0m parent \u001B[38;5;241m=\u001B[39m Path(path)\u001B[38;5;241m.\u001B[39mparent\n\u001B[1;32m    596\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m parent\u001B[38;5;241m.\u001B[39mis_dir():\n\u001B[0;32m--> 597\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\u001B[38;5;124mrf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot save file into a non-existent directory: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparent\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "\u001B[0;31mOSError\u001B[0m: Cannot save file into a non-existent directory: '/dbfs/FileStore'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"OrderID\": [1, 2, 3, 4],\n",
    "    \"OrderDate\": [\"2024-01-01 10:00:00\", \"2024-01-02 11:00:00\", \"2024-01-03 12:00:00\", \"2024-01-04 13:00:00\"],\n",
    "    \"CustomerID\": [\"C001\", \"C002\", \"C003\", \"C004\"],\n",
    "    \"Product\": [\"ProductA\", \"ProductB\", \"ProductC\", \"ProductD\"],\n",
    "    \"Quantity\": [10, 20, 15, 5],\n",
    "    \"Price\": [100.0, 200.0, 150.0, 50.0]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to csv\n",
    "csv_path = \"/dbfs/FileStore/sales2_data.csv\"\n",
    "df.to_csv(csv_path,index=False)\n",
    "\n",
    "print(f\"Sample data save to {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c12888e-843e-4e90-889c-15b693b86c3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"StructuredStreamingExample\").getOrCreate()\n",
    "\n",
    "# Load data from CSV\n",
    "df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"/FileStore/sales2_data.csv\")\n",
    "\n",
    "# Transform daat adding a column\n",
    "df_transformed = df.withColumn(\"TotalAmount\",col(\"Quantity\").cast(\"int\") * col(\"Price\").cast(\"double\") )\n",
    "\n",
    "# Write transformed data to a delta table\n",
    "df_transformed.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/sales2_data\")\n",
    "\n",
    "print(\"Transformed data written into delta table\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data processing - Job",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
