{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaseqSX9YSDB"
      },
      "outputs": [],
      "source": [
        "# Used Google Colab\n",
        "\n",
        "#  Exercise 1: Creating a Complete ETL Pipeline using Delta Live Tables\n",
        "# DLT in python\n",
        "\n",
        "import dlt\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "@dlt.table\n",
        "def raw_transactions():\n",
        "    return (\n",
        "        spark.read.format(\"csv\")\n",
        "        .option(\"header\", \"true\")\n",
        "        .load(\"/dbfs/FileStore/transactions.csv\")\n",
        "    )\n",
        "\n",
        "@dlt.table\n",
        "def transformed_transactions():\n",
        "    return (\n",
        "        dlt.read(\"raw_transactions\")\n",
        "        .withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"Price\"))\n",
        "    )\n",
        "\n",
        "@dlt.table\n",
        "def final_transactions():\n",
        "    return dlt.read(\"transformed_transactions\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DLT in SQL\n",
        "\n",
        "CREATE OR REPLACE LIVE TABLE raw_transactions AS\n",
        "SELECT *\n",
        "FROM read_csv('/dbfs/FileStore/transactions.csv');\n",
        "\n",
        "CREATE OR REPLACE LIVE TABLE transformed_transactions AS\n",
        "SELECT *, Quantity * Price AS TotalAmount\n",
        "FROM raw_transactions;\n",
        "\n",
        "CREATE OR REPLACE LIVE TABLE final_transactions AS\n",
        "SELECT *\n",
        "FROM transformed_transactions;\n"
      ],
      "metadata": {
        "id": "feAGSipeZb5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Exercise 2: Delta Lake Operations - Read, Write, Update, Delete, Merge\n",
        "\n",
        "# Task 1 Read Data from Delta lake\n",
        "# python\n",
        "delta_df = spark.read.format(\"delta\").load(\"/delta/final_transactions\")\n",
        "\n",
        "delta_df.show(5)\n",
        "\n",
        "# sql\n",
        "spark.sql(\"\"\"SELECT *\n",
        "FROM delta.`/delta/final_transactions`\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "neHnxCMbZ3KO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2 Write data to delta\n",
        "\n",
        "new_transactions = [\n",
        "    (6, '2024-09-06', 'C005', 'Keyboard', 4, 100),\n",
        "    (7, '2024-09-07', 'C006', 'Mouse', 10, 20)\n",
        "]\n",
        "\n",
        "new_transactions_df = spark.createDataFrame(new_transactions,\n",
        "    schema=[\"TransactionID\", \"TransactionDate\", \"CustomerID\", \"Product\", \"Quantity\", \"Price\"])\n",
        "\n",
        "new_transactions_df.write.format(\"delta\").mode(\"append\").save(\"/delta/final_transactions\")\n"
      ],
      "metadata": {
        "id": "1qWIR2RnaVla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3 Update data in delta\n",
        "# python\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "updated_df = delta_df.withColumn(\n",
        "    \"Price\",\n",
        "    when(col(\"Product\") == \"Laptop\", 1300).otherwise(col(\"Price\"))\n",
        ")\n",
        "\n",
        "updated_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/final_transactions\")\n",
        "\n",
        "# sql\n",
        "spark.sql(\"\"\"UPDATE delta.`/delta/final_transactions`\n",
        "SET Price = 1300\n",
        "WHERE Product = 'Laptop';\"\"\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "65JHxUuiabvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4 Delete from delta\n",
        "# python\n",
        "\n",
        "delta_df = delta_df.filter(col(\"Quantity\") >= 3)\n",
        "\n",
        "delta_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/final_transactions\")\n",
        "\n",
        "#sql\n",
        "spark.sql(\"\"\"DELETE FROM delta.`/delta/final_transactions`\n",
        "WHERE Quantity < 3;\"\"\")\n",
        "\n"
      ],
      "metadata": {
        "id": "_foL4EhNaqG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5 Merge data into delta\n",
        "spark.sql(\"\"\"\n",
        "MERGE INTO delta.`/delta/final_transactions` AS existing\n",
        "USING (\n",
        "    SELECT * FROM VALUES\n",
        "    (1, '2024-09-01', 'C001', 'Laptop', 1, 1250),\n",
        "    (8, '2024-09-08', 'C007', 'Charger', 2, 30)\n",
        ") AS updates (TransactionID, TransactionDate, CustomerID, Product, Quantity, Price)\n",
        "ON existing.TransactionID = updates.TransactionID\n",
        "WHEN MATCHED THEN\n",
        "    UPDATE SET\n",
        "        existing.TransactionDate = updates.TransactionDate,\n",
        "        existing.CustomerID = updates.CustomerID,\n",
        "        existing.Product = updates.Product,\n",
        "        existing.Quantity = updates.Quantity,\n",
        "        existing.Price = updates.Price\n",
        "WHEN NOT MATCHED THEN\n",
        "    INSERT (TransactionID, TransactionDate, CustomerID, Product, Quantity, Price)\n",
        "    VALUES (updates.TransactionID, updates.TransactionDate, updates.CustomerID, updates.Product, updates.Quantity, updates.Price);\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "RodXoq86bBr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3 History, time travel, vacuum\n",
        "\n",
        "# Task 1 history of the delta table\n",
        "history_df = spark.sql(\"DESCRIBE HISTORY delta.`/delta/final_transactions`\")\n",
        "history_df.show()\n",
        "\n",
        "spark.sql(\"DESCRIBE HISTORY delta.`/delta/final_transactions`;\")\n"
      ],
      "metadata": {
        "id": "-P-U__JsbZtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2 Time travel\n",
        "\n",
        "time_travel_df = spark.read.format(\"delta\").option(\"versionAsOf\", 5).load(\"/delta/final_transactions\")\n",
        "time_travel_df.show()\n",
        "\n",
        "spark.sql(\"SELECT * FROM delta.`/delta/final_transactions` VERSION AS OF 5;\")\n",
        "\n",
        "# using timestamp\n",
        "\n",
        "spark.sql(\"SELECT * FROM delta.`/delta/final_transactions` TIMESTAMP AS OF '2024-09-06 00:00:00';\")\n"
      ],
      "metadata": {
        "id": "fFKjFrqAbq0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3 Vacuum\n",
        "\n",
        "spark.sql(\"VACUUM delta.`/delta/final_transactions` RETAIN 168 HOURS\")"
      ],
      "metadata": {
        "id": "FLzZzkgXcPXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4 Parquet to delta\n",
        "\n",
        "csv_data = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/dbfs/FileStore/transactions.csv\")\n",
        "csv_data.write.format(\"parquet\").mode(\"overwrite\").save(\"/dbfs/FileStore/transactions_parquet\")\n",
        "\n",
        "parquet_data = spark.read.format(\"parquet\").load(\"/dbfs/FileStore/transactions_parquet\")\n",
        "parquet_data.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/transactions_delta\")\n",
        "\n"
      ],
      "metadata": {
        "id": "SG_bz-MGchUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Exercise 4: Implementing Incremental Load Pattern using Delta Lake\n",
        "\n",
        "# Task 1 Setup inital data\n",
        "\n",
        "initial_data = [\n",
        "    (1, \"2024-09-01\", \"C001\", \"Laptop\", 1, 1200),\n",
        "    (2, \"2024-09-02\", \"C002\", \"Tablet\", 2, 300),\n",
        "    (3, \"2024-09-03\", \"C001\", \"Headphones\", 5, 50)\n",
        "]\n",
        "\n",
        "columns = [\"TransactionID\", \"TransactionDate\", \"CustomerID\", \"Product\", \"Quantity\", \"Price\"]\n",
        "\n",
        "initial_df = spark.createDataFrame(initial_data, columns)\n",
        "\n",
        "initial_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/transactions\")\n"
      ],
      "metadata": {
        "id": "iCWNY3Au8hlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2 Setup incremental data\n",
        "incremental_data = [\n",
        "    (4, \"2024-09-04\", \"C003\", \"Smartphone\", 1, 800),\n",
        "    (5, \"2024-09-05\", \"C004\", \"Smartwatch\", 3, 200),\n",
        "    (6, \"2024-09-06\", \"C005\", \"Keyboard\", 4, 100),\n",
        "    (7, \"2024-09-07\", \"C006\", \"Mouse\", 10, 20)\n",
        "]\n",
        "\n",
        "incremental_df = spark.createDataFrame(incremental_data, columns)\n"
      ],
      "metadata": {
        "id": "mPqBG8zI8wRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3 Implement Incremental load\n",
        "\n",
        "# transactions after 2024-09-03\n",
        "incremental_load_df = incremental_df.filter(incremental_df.TransactionDate > \"2024-09-03\")\n",
        "\n",
        "incremental_load_df.write.format(\"delta\").mode(\"append\").save(\"/delta/transactions\")\n",
        "\n",
        "full_data_df = spark.read.format(\"delta\").load(\"/delta/transactions\")\n",
        "full_data_df.show()\n",
        "\n",
        "history_df = spark.sql(\"DESCRIBE HISTORY delta.`/delta/transactions`\")\n",
        "history_df.show()\n"
      ],
      "metadata": {
        "id": "3Dkjrk1e85-j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}