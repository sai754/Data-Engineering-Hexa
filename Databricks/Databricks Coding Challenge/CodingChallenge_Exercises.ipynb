{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqbH5Cu5Ij2c"
      },
      "outputs": [],
      "source": [
        "# Used Google Colab\n",
        "\n",
        "# Exercise 1\n",
        "# Task 1\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "import os\n",
        "\n",
        "spark = SparkSession.builder.appName(\"VehicleMaintenanceDataIngestion\").getOrCreate()\n",
        "\n",
        "schema = \"VehicleID STRING, Date DATE, ServiceType STRING, ServiceCost FLOAT, Mileage INT\"\n",
        "\n",
        "csv_file_path = \"/dbfs/FileStore/vehicle_maintenance.csv\"\n",
        "\n",
        "try:\n",
        "    if not os.path.exists(csv_file_path):\n",
        "        raise FileNotFoundError(\"File not found at path\")\n",
        "\n",
        "    vehicle_df = spark.read.csv(csv_file_path, schema=schema, header=True)\n",
        "\n",
        "    if vehicle_df.filter(F.col(\"VehicleID\").isNull() | F.col(\"Date\").isNull()).count() > 0:\n",
        "        print(\"Data contains missing VehicleID or Date values\")\n",
        "    else:\n",
        "        vehicle_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/vehicle_maintenance\")\n",
        "\n",
        "        print(\"Data ingestion completed\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(e)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during ingestion: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2\n",
        "vehicle_df = spark.read.format(\"delta\").load(\"/delta/vehicle_maintenance\")\n",
        "\n",
        "cleaned_df = vehicle_df.filter((F.col(\"ServiceCost\") > 0) & (F.col(\"Mileage\") > 0))\n",
        "\n",
        "cleaned_df = cleaned_df.dropDuplicates([\"VehicleID\"])\n",
        "\n",
        "cleaned_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/cleaned_vehicle_maintenance\")\n",
        "\n",
        "print(\"data cleaned\")\n"
      ],
      "metadata": {
        "id": "z12H8lHNLP6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3\n",
        "cleaned_vehicle_df = spark.read.format(\"delta\").load(\"/delta/cleaned_vehicle_maintenance\")\n",
        "\n",
        "total_cost_df = cleaned_vehicle_df.groupBy(\"VehicleID\") \\\n",
        "                                  .agg(F.sum(\"ServiceCost\").alias(\"TotalMaintenanceCost\"))\n",
        "\n",
        "high_mileage_df = cleaned_vehicle_df.filter(F.col(\"Mileage\") > 30000) \\\n",
        "                                    .select(\"VehicleID\", \"Mileage\") \\\n",
        "                                    .distinct()\n",
        "\n",
        "total_cost_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/total_cost\")\n",
        "\n",
        "high_mileage_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/high_mileage\")\n",
        "\n",
        "print(\"Vehicle maintenance analysis completed\")"
      ],
      "metadata": {
        "id": "9PXUZ7-8Lpqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4\n",
        "\n",
        "spark.sql(\"VACUUM '/delta/vehicle_maintenance_analysis' RETAIN 168 HOURS\")\n",
        "\n",
        "spark.sql(\"DESCRIBE HISTORY '/delta/vehicle_maintenance_analysis'\")"
      ],
      "metadata": {
        "id": "UgeWoV20PI-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2\n",
        "\n",
        "# Task 1\n",
        "from pyspark.sql import SparkSession\n",
        "import os\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark = SparkSession.builder.appName(\"MovieRatingsDataIngestion\").getOrCreate()\n",
        "\n",
        "schema = \"UserID STRING, MovieID STRING, Rating INT, Timestamp STRING\"\n",
        "\n",
        "csv_file_path = \"/dbfs/FileStore/movie_ratings.csv\"\n",
        "\n",
        "try:\n",
        "    if not os.path.exists(csv_file_path):\n",
        "        raise FileNotFoundError(\"File not found\")\n",
        "\n",
        "    ratings_df = spark.read.csv(csv_file_path, schema=schema, header=True)\n",
        "\n",
        "    inconsistent_data = ratings_df.filter(F.col(\"UserID\").isNull() |\n",
        "                                          F.col(\"MovieID\").isNull() |\n",
        "                                          F.col(\"Rating\").isNull() |\n",
        "                                          (F.col(\"Rating\") < 1) |\n",
        "                                          (F.col(\"Rating\") > 5))\n",
        "\n",
        "    if inconsistent_data.count() > 0:\n",
        "        print(\"missing data found\")\n",
        "    else:\n",
        "        ratings_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/movie_ratings\")\n",
        "\n",
        "        print(\"Data ingestion successful\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(e)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during ingestion: {e}\")\n"
      ],
      "metadata": {
        "id": "IJBOCLOQPT4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2\n",
        "ratings_df = spark.read.format(\"delta\").load(\"/delta/movie_ratings\")\n",
        "\n",
        "cleaned_df = ratings_df.filter((F.col(\"Rating\") >= 1) & (F.col(\"Rating\") <= 5))\n",
        "\n",
        "cleaned_df = cleaned_df.dropDuplicates([\"UserID\", \"MovieID\"])\n",
        "\n",
        "cleaned_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/cleaned_movie_ratings\")\n",
        "\n",
        "print(\"Movie ratings data cleaned\")"
      ],
      "metadata": {
        "id": "8fwnFk55RAvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3\n",
        "ratings_df = spark.read.format(\"delta\").load(\"/delta/cleaned_movie_ratings\")\n",
        "\n",
        "avg_ratings_df = ratings_df.groupBy(\"MovieID\").agg(F.avg(\"Rating\").alias(\"AverageRating\"))\n",
        "\n",
        "highest_rated_movie = avg_ratings_df.orderBy(F.col(\"AverageRating\").desc()).limit(1)\n",
        "lowest_rated_movie = avg_ratings_df.orderBy(F.col(\"AverageRating\").asc()).limit(1)\n",
        "\n",
        "avg_ratings_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/movie_ratings_analysis\")\n",
        "highest_rated_movie.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/highest_rated\")\n",
        "lowest_rated_movie.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/lowest_rated\")\n",
        "\n",
        "print(\"Movie ratings analysis saved\")"
      ],
      "metadata": {
        "id": "D44ObK0VRcXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4\n",
        "\n",
        "ratings_df = spark.read.format(\"delta\").load(\"/delta/cleaned_movie_ratings\")\n",
        "updated_ratings_df = ratings_df.withColumn(\"Rating\", F.when(F.col(\"MovieID\") == \"M001\", 5).otherwise(F.col(\"Rating\")))\n",
        "\n",
        "updated_ratings_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/cleaned_movie_ratings\")\n",
        "\n",
        "original_ratings_df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/delta/cleaned_movie_ratings\")\n",
        "\n",
        "spark.sql(\"DESCRIBE HISTORY '/delta/cleaned_movie_ratings'\")"
      ],
      "metadata": {
        "id": "a2GEfaVkSPil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5\n",
        "\n",
        "spark.sql(\"OPTIMIZE '/delta/cleaned_movie_ratings' ZORDER BY (MovieID)\")\n",
        "\n",
        "spark.sql(\"VACUUM '/delta/cleaned_movie_ratings' RETAIN 168 HOURS\")\n"
      ],
      "metadata": {
        "id": "o4KRDCNGShAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3\n",
        "# Task 1\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"DataIngestion\").getOrCreate()\n",
        "\n",
        "csv_data = [(\"S001\", \"Anil Kumar\", 10, 85),\n",
        "            (\"S002\", \"Neha Sharma\", 12, 92),\n",
        "            (\"S003\", \"Rajesh Gupta\", 11, 78)]\n",
        "\n",
        "csv_df = spark.createDataFrame(csv_data, [\"StudentID\", \"Name\", \"Class\", \"Score\"])\n",
        "\n",
        "json_data = [\n",
        "    {\"CityID\": \"C001\", \"CityName\": \"Mumbai\", \"Population\": 20411000},\n",
        "    {\"CityID\": \"C002\", \"CityName\": \"Delhi\", \"Population\": 16787941},\n",
        "    {\"CityID\": \"C003\", \"CityName\": \"Bangalore\", \"Population\": 8443675}\n",
        "]\n",
        "\n",
        "json_df = spark.read.json(json_data)\n",
        "\n",
        "parquet_df = spark.read.parquet(\"/path/to/hospital_data.parquet\")\n",
        "\n",
        "try:\n",
        "    delta_df = spark.read.format(\"delta\").load(\"/delta/hospital_records\")\n",
        "except Exception as e:\n",
        "    print(\"Error loading Delta table\")\n"
      ],
      "metadata": {
        "id": "pMUR5sRWTZ2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2\n",
        "csv_df.write.csv(\"/dbfs/FileStore/student_data.csv\", header=True)\n",
        "\n",
        "json_df.write.json(\"/dbfs/FileStore/city_data.json\")\n",
        "\n",
        "parquet_df.write.parquet(\"/dbfs/FileStore/hospital_data.parquet\")\n",
        "\n",
        "parquet_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/hospital_data\")\n"
      ],
      "metadata": {
        "id": "33P2huk-ToXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3\n",
        "# Notebook one\n",
        "\n",
        "student_df = spark.read.csv(\"/dbfs/FileStore/student_data.csv\", header=True)\n",
        "\n",
        "cleaned_student_df = student_df.dropDuplicates().na.fill({\"Score\": 0})\n",
        "\n",
        "cleaned_student_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/cleaned_student_data\")\n",
        "\n",
        "dbutils.notebook.run(\"/Workspace/user/Notebook_B\", 60)\n"
      ],
      "metadata": {
        "id": "1mYal7lNUECm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Notebook b\n",
        "cleaned_student_df = spark.read.format(\"delta\").load(\"/delta/cleaned_student_data\")\n",
        "\n",
        "average_score_df = cleaned_student_df.groupBy(\"Class\").agg(F.avg(\"Score\").alias(\"AverageScore\"))\n",
        "\n",
        "average_score_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/average_student_scores\")\n"
      ],
      "metadata": {
        "id": "dt4bhnkWUQYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4\n",
        "azure_df = spark.read.csv(\"abfss://account/data/data.csv\", header=True)\n",
        "\n",
        "databricks_json_df = spark.read.json(\"/FileStore/path/to/data.json\")\n",
        "\n",
        "s3_parquet_df = spark.read.parquet(\"s3://<bucket>/data.parquet\")\n",
        "\n",
        "delta_table_df = spark.read.format(\"delta\").load(\"/delta/databricks_table\")\n",
        "\n",
        "cleaned_df = azure_df.filter(F.col(\"Score\") > 50)\n",
        "\n",
        "total_score_df = cleaned_df.groupBy(\"Class\").agg(F.sum(\"Score\").alias(\"TotalScore\"))\n",
        "\n",
        "\n",
        "cleaned_df.write.csv(\"/dbfs/FileStore/cleaned_data.csv\", header=True)\n",
        "cleaned_df.write.json(\"/dbfs/FileStore/cleaned_data.json\")\n",
        "cleaned_df.write.parquet(\"/dbfs/FileStore/cleaned_data.parquet\")\n",
        "cleaned_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/cleaned_data\")\n",
        "\n",
        "total_score_df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/total_student_scores\")\n"
      ],
      "metadata": {
        "id": "GFR_gGX_Ux8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional Tasks\n",
        "spark.sql(\"OPTIMIZE '/delta/cleaned_data'\")\n",
        "\n",
        "spark.sql(\"OPTIMIZE '/delta/cleaned_data' ZORDER BY (CityName)\")\n",
        "\n",
        "spark.sql(\"VACUUM '/delta/cleaned_data' RETAIN 168 HOURS\")\n"
      ],
      "metadata": {
        "id": "ME9yQUBfV0Mx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}