{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4605688-e139-4aff-9722-588ba108cd41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales data successfully written to: /dbfs/FileStore/streaming/input/sales_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Sample data in Python list\n",
    "data = [\n",
    "    (1001, \"2024-01-15\", \"C001\", \"Widget A\", 10, 25.50),\n",
    "    (1002, \"2024-01-16\", \"C002\", \"Widget B\", 5, 15.75),\n",
    "    (1003, \"2024-01-16\", \"C001\", \"Widget C\", 8, 22.50),\n",
    "    (1004, \"2024-01-17\", \"C003\", \"Widget A\", 15, 25.50),\n",
    "    (1005, \"2024-01-18\", \"C004\", \"Widget D\", 7, 30.00),\n",
    "    (1006, \"2024-01-19\", \"C002\", \"Widget B\", 9, 15.75),\n",
    "    (1007, \"2024-01-20\", \"C005\", \"Widget C\", 12, 22.50),\n",
    "    (1008, \"2024-01-21\", \"C003\", \"Widget A\", 10, 25.50)\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "columns = [\"OrderID\", \"OrderDate\", \"CustomerID\", \"Product\", \"Quantity\", \"Price\"]\n",
    "\n",
    "# Create Spark DataFrame\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "# Save as CSV file to DBFS (Databricks File System)\n",
    "output_path_csv = \"/dbfs/FileStore/streaming/input/sales_data.csv\"\n",
    "\n",
    "df.write.option(\"header\", \"true\").csv(output_path_csv)\n",
    "\n",
    "print(\"Sales data successfully written to:\", output_path_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b10644b-9a99-46cc-8ba8-edabafaf17b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer data successfully written to: /dbfs/FileStore/streaming/input/customer_data.json\n"
     ]
    }
   ],
   "source": [
    "# JSON data as Python list of dictionaries\n",
    "customer_data = [\n",
    "    {\"CustomerID\": \"C001\", \"CustomerName\": \"John Doe\", \"Region\": \"North\", \"SignupDate\": \"2022-07-01\"},\n",
    "    {\"CustomerID\": \"C002\", \"CustomerName\": \"Jane Smith\", \"Region\": \"South\", \"SignupDate\": \"2023-02-15\"},\n",
    "    {\"CustomerID\": \"C003\", \"CustomerName\": \"Emily Johnson\", \"Region\": \"East\", \"SignupDate\": \"2021-11-20\"},\n",
    "    {\"CustomerID\": \"C004\", \"CustomerName\": \"Michael Brown\", \"Region\": \"West\", \"SignupDate\": \"2022-12-05\"},\n",
    "    {\"CustomerID\": \"C005\", \"CustomerName\": \"Linda Davis\", \"Region\": \"North\", \"SignupDate\": \"2023-03-10\"}\n",
    "]\n",
    "\n",
    "# Create Spark DataFrame\n",
    "df_customer = spark.createDataFrame(customer_data)\n",
    "\n",
    "# Save as JSON file to DBFS (Databricks File System)\n",
    "output_path_json = \"/dbfs/FileStore/streaming/input/customer_data.json\"\n",
    "\n",
    "df_customer.write.mode(\"overwrite\").json(output_path_json)\n",
    "\n",
    "print(\"Customer data successfully written to:\", output_path_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1cc05c7-dd7e-491b-bb0d-894002149925",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.fs.cp(\"file:/Workspace/Shared/sales_data.csv\",\"dbfs:/FileStore/streaming/input/sales_data.csv\")\n",
    "\n",
    "# dbutils.fs.cp(\"file:/Workspace/Shared/customer_data.json\",\"dbfs:/FileStore/streaming/input/customer_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c965a44-51ee-4143-b9e3-eeb175f77dd9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"StructuredStreaming\").getOrCreate()\n",
    "\n",
    "# Define the Schema for the csv data\n",
    "sales_schema = \"OrderID INT, OrderDate STRING, CustomerID STRING, Product STRING, Quantity INT, Price DOUBLE\"\n",
    "\n",
    "# Read streaming dat from csv files\n",
    "df_sales_stream = spark.readStream.format(\"csv\").option(\"header\",\"true\").schema(sales_schema).load(\"/dbfs/FilesStore/streaming/input/sales_data.csv\")\n",
    "\n",
    "# Define the Schema for json data\n",
    "customer_schema = \"CustomerID STRING, CustomerName STRING, Region STRING, SignupDate STRING\"\n",
    "\n",
    "# Read streaming dat from json file\n",
    "df_customer_stream = spark.readStream.format(\"json\").schema(customer_schema).load(\"/dbfs/FileStore/streaming/input/customer_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2da808a0-5201-47a7-b9e0-5ac3f7c85f17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the sales data stream to the console\n",
    "sales_query =  df_sales_stream.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f27ca9ef-5ff3-4cec-ad25-c7cf6e02fd8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f87eb4e9-278e-4518-81a3-6032b4fa9f36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied transformation on sales data\nAggregated sales data by product\nApplied transformations on customer data\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date, datediff,to_timestamp\n",
    "\n",
    "# Transform the sales data: adda anew column for total amount\n",
    "df_sales_transformed = df_sales_stream.select(\n",
    "    col(\"OrderID\"),\n",
    "    to_timestamp(col(\"OrderDate\"),\"yyyy-MM-dd HH:mm:ss\").alias(\"OrderDate\"), # Convert OrderDate to timestamp\n",
    "    col(\"Product\"),\n",
    "    col(\"Quantity\"),\n",
    "    col(\"Price\"),\n",
    "    (col(\"Quantity\") * col(\"Price\")).alias(\"TotalAmount\")\n",
    ")\n",
    "\n",
    "print(\"Applied transformation on sales data\")\n",
    "\n",
    "# Add Watermark to handle late data and perform an aggregatino\n",
    "df_sales_aggregated = df_sales_transformed.withWatermark(\"OrderDate\",\"1 day\").groupBy(\"Product\").agg({\"TotalAmount\":\"sum\"})\n",
    "\n",
    "print(\"Aggregated sales data by product\")\n",
    "\n",
    "# Transform the customer data: adda anew column of years since signup\n",
    "df_customer_transformed = df_customer_stream.withColumn(\"YearsSinceSignup\",datediff(current_date(),to_timestamp(col(\"SignupDate\"),'yyyy-MM--dd')).cast(\"int\") / 365\n",
    ")\n",
    "\n",
    "print(\"Applied transformations on customer data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e74419d-7d3b-49b0-86bf-4c08e4c94c6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Streaming query to write aggregated sales data to console\n"
     ]
    }
   ],
   "source": [
    "# Write the aggregated sales data to a console sink for debugging\n",
    "sales_query = df_sales_aggregated.writeStream.outputMode(\"update\").format(\"console\").start()\n",
    "\n",
    "print(\"Started Streaming query to write aggregated sales data to console\")\n",
    "\n",
    "# Write the transformed customer data to a console sink \n",
    "customer_query = df_customer_transformed.writeStream.outputMode(\"append\").format(\"console\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a35f7ce-745c-4489-9ad4-bc531d64a68e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales_query.stop()\n",
    "customer_query.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3848785157626266,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Streaming_Data",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
