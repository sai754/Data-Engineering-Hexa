{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f05d08d-4da8-4d75-99f9-4abb92c3884c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move the file from workspace to DBFS\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/employee1_data.csv\", \"dbfs:/employee1_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28ed8fa5-8d4e-43ec-8d8e-b5c4038186e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the csv data into a dataframe\n",
    "df_employee = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"dbfs:/FileStore/employee1_data.csv\")\n",
    "\n",
    "\n",
    "# Wrtie the dataframe in delta format\n",
    "df_employee.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/employee1_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27b5273a-57b8-49ed-93ca-357f48313483",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------+------+-----+\n",
      "|ProductID|ProductName|   Category| Price|Stock|\n",
      "+---------+-----------+-----------+------+-----+\n",
      "|      101|     Laptop|Electronics|1200.0|   35|\n",
      "|      102| Smartphone|Electronics| 800.0|   80|\n",
      "|      103| Desk Chair|  Furniture| 150.0|   60|\n",
      "|      104|    Monitor|Electronics| 300.0|   45|\n",
      "|      105|       Desk|  Furniture| 350.0|   25|\n",
      "+---------+-----------+-----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"ProductID\", IntegerType(), True),\n",
    "    StructField(\"ProductName\", StringType(), True),\n",
    "    StructField(\"Category\", StringType(), True),\n",
    "    StructField(\"Price\", DoubleType(), True),\n",
    "    StructField(\"Stock\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "# Move the file from workspace to DBFS\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/product1_data.json\", \"dbfs:/FileStore/product1_data.json\")\n",
    "\n",
    "# Load JSON data with schema\n",
    "df_product = spark.read.schema(schema).json(\"dbfs:/FileStore/product1_data.json\")\n",
    "df_product.show()\n",
    "\n",
    "# Create a temp view for SQL operations\n",
    "df_product.createOrReplaceTempView(\"product_view\")\n",
    "\n",
    "# Create Delta Table from the view\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS product1_data USING DELTA AS SELECT * FROM product_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the file from Workspace to DBFS\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/employee_updated.csv\", \"dbfs:/FileStore/employee_updated.csv\")\n",
    "\n",
    "\n",
    "# Convert employee csv data to Delta Format\n",
    "# Load the csv data into a dataframe\n",
    "df_employee1 = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"dbfs:/FileStore/employee1_data.csv\")\n",
    "\n",
    "\n",
    "# Wrtie the dataframe in delta format\n",
    "df_employee1.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/employee1_data\")\n",
    "\n",
    "\n",
    "# Convert employee updates csv to delta format\n",
    "df_employee_updates = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"/FileStore/employee_updated.csv\")\n",
    "df_employee_updates.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/employee_updates\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Delta tables\n",
    "df_emp = spark.read.format(\"delta\").load(\"/delta/employee_data\")\n",
    "df_emp_updates = spark.read.format(\"delta\").load(\"/delta/employee_updates\")\n",
    "\n",
    "\n",
    "# Create a temporary view for SQL operations\n",
    "df_emp.createOrReplaceTempView(\"delta_employee\")\n",
    "df_emp_updates.createOrReplaceTempView(\"employee_updates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    MERGE INTO delta_employee AS target\n",
    "    USING employee_updates AS source\n",
    "    ON target.EmployeeID = source.EmployeeID\n",
    "    WHEN MATCHED THEN UPDATE SET target.Salary = source.Salary, target.Department = source.Department\n",
    "    WHEN NOT MATCHED THEN INSERT (EmployeeID, Name, Department, JoiningDate, Salary)\n",
    "    VALUES (source.EmployeeID, source.Name, source.Department, source.JoiningDate, source.Salary)\n",
    "\"\"\")\n",
    "\n",
    "# Query the Delta table to check if the data was updated or inserted correctly\n",
    "spark.sql(\"SELECT * FROM delta_employee\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the employee dataframe to a delta table\n",
    "df_emp.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/employee_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the delta table\n",
    "spark.sql(\"Create table if not exists delta_employee_table using DELTA Location '/delta/employee_data'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the delta table\n",
    "spark.sql(\"OPTIMIZE delta_employee_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the history of the Delta table\n",
    "spark.sql(\"DESCRIBE HISTORY delta_employee_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZORDER the delta table\n",
    "spark.sql(\"OPTIMIZE delta_employee_table ZORDER BY Department\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE records older than a week\n",
    "spark.sql(\"VACUUM delta_employee_table RETAIN 168 HOURS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "--In data bricks notebook u can change cell to sql cell\n",
    "\n",
    "%sql\n",
    "CREATE TABLE managed_table {\n",
    "    id INT,\n",
    "    name String\n",
    "};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE EXTERNAL TABLE unmanaged_table {\n",
    "    id INT,\n",
    "    name STRING\n",
    "}\n",
    "LOCATION '/user/data/external_data/';"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "delta-table",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
