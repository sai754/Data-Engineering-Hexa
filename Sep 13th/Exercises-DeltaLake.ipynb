{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71793e99-1509-4968-b437-7431e3e92fd8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 1\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/sales1_data.csv\", \"dbfs:/FileStore/sales1_data.csv\")\n",
    "\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/customer_data.json\", \"dbfs:/FileStore/customer_data.json\")\n",
    "\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/new_sales_data.csv\", \"dbfs:/FileStore/new_sales_data.csv\")\n",
    "\n",
    "\n",
    "# Load it into a DataFrame\n",
    "df_sales = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"dbfs:/FileStore/sales1_data.csv\")\n",
    "\n",
    "df_customers = spark.read.option(\"multiline\", \"true\").json(\"dbfs:/FileStore/customer_data.json\")\n",
    "\n",
    "# Write the Dataframe as delta table\n",
    "df_sales.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(\"dbfs:/FileStore/sales_delta\")\n",
    "\n",
    "\n",
    "df_customers.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/FileStore/customers_delta\")\n",
    "\n",
    "# Convert Parquet file into delta table\n",
    "df_sales.write.format(\"parquet\").mode(\"overwrite\").save(\"dbfs:/FileStore/sales_parquet\")\n",
    "\n",
    "df_parquet = spark.read.format(\"parquet\").load(\"dbfs:/FileStore/sales_parquet\")\n",
    "\n",
    "df_parquet.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/FileStore/sales_delta_parquet\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e147a85b-945e-47b8-aca8-dfb43afdd311",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+--------+--------+-----+\n|OrderID| OrderDate|CustomerID| Product|Quantity|Price|\n+-------+----------+----------+--------+--------+-----+\n|   1001|2024-01-15|      C001|Widget A|      10|25.50|\n|   1003|2024-01-16|      C001|Widget C|       8|22.50|\n|   1004|2024-01-17|      C003|Widget A|      15|25.50|\n|   1005|2024-01-18|      C004|Widget D|       7|30.00|\n|   1006|2024-01-19|      C002|Widget B|       9|15.75|\n|   1007|2024-01-20|      C005|Widget C|      12|22.50|\n|   1008|2024-01-21|      C003|Widget A|      10|25.50|\n|   1002|2024-01-16|      C002|Widget B|      10|15.75|\n|   1009|2024-01-22|      C006|Widget E|      14|20.00|\n|   1010|2024-01-23|      C007|Widget F|       6|35.00|\n+-------+----------+----------+--------+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Task 2\n",
    "# Load new sales into the daata frame\n",
    "df_new_sales = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"dbfs:/FileStore/new_sales_data.csv\")\n",
    "\n",
    "# Write into delta table\n",
    "df_new_sales.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(\"dbfs:/FileStore/new_sales_delta\")\n",
    "\n",
    "\n",
    "sales_df = spark.read.format(\"delta\").load(\"dbfs:/FileStore/sales_delta\")\n",
    "new_sales_df = spark.read.format(\"delta\").load(\"dbfs:/FileStore/new_sales_delta\")\n",
    "\n",
    "sales_df.createOrReplaceTempView(\"delta_sales\")\n",
    "new_sales_df.createOrReplaceTempView(\"delta_new_sales\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "        MERGE INTO delta_sales as target\n",
    "        USING delta_new_sales as source\n",
    "        ON target.OrderID = source.OrderID\n",
    "        WHEN MATCHED THEN\n",
    "        UPDATE SET\n",
    "                target.OrderDate = source.OrderDate,\n",
    "                target.CustomerID = source.CustomerID,\n",
    "                target.Product = source.Product,\n",
    "                target.Quantity = source.Quantity,\n",
    "                target.Price = source.Price\n",
    "        WHEN NOT MATCHED THEN\n",
    "        INSERT (OrderID, OrderDate, CustomerID, Product, Quantity, Price)\n",
    "        VALUES (source.OrderID, source.OrderDate, source.CustomerID, source.Product, source.Quantity, source.Price);  \n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM delta_sales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58e0d26f-8bb9-4dde-b1a1-53f59ccf0035",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n|                path|             metrics|\n+--------------------+--------------------+\n|dbfs:/FileStore/s...|{0, 0, {NULL, NUL...|\n+--------------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Task 3\n",
    "# Register the Delta table\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS sales_delta_table USING DELTA LOCATION 'dbfs:/FileStore/sales_delta'\")\n",
    "\n",
    "# Optimize\n",
    "spark.sql(\"OPTIMIZE sales_delta_table ZORDER BY (CustomerID)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "466885d5-6d50-4ef6-b538-75ee874a5223",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|version|          timestamp|          userId|            userName|operation| operationParameters| job|          notebook|           clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n+-------+-------------------+----------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|      9|2024-09-13 07:16:19|6822499424790006|azuser2121_mml.lo...| OPTIMIZE|{predicate -> [],...|NULL|{3562288518827637}|0911-073432-r1idfcx3|          8|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|      8|2024-09-13 07:16:16|6822499424790006|azuser2121_mml.lo...|    MERGE|{predicate -> [\"(...|NULL|{3562288518827637}|0911-073432-r1idfcx3|          7|WriteSerializable|        false|{numTargetRowsCop...|        NULL|Databricks-Runtim...|\n|      7|2024-09-13 07:16:02|6822499424790006|azuser2121_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{3562288518827637}|0911-073432-r1idfcx3|          6|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n|      6|2024-09-13 07:00:11|6822499424790006|azuser2121_mml.lo...| OPTIMIZE|{predicate -> [],...|NULL|{3562288518827637}|0911-073432-r1idfcx3|          5|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|      5|2024-09-13 07:00:09|6822499424790006|azuser2121_mml.lo...|    MERGE|{predicate -> [\"(...|NULL|{3562288518827637}|0911-073432-r1idfcx3|          4|WriteSerializable|        false|{numTargetRowsCop...|        NULL|Databricks-Runtim...|\n|      4|2024-09-13 06:59:51|6822499424790006|azuser2121_mml.lo...| OPTIMIZE|{predicate -> [],...|NULL|{3562288518827637}|0911-073432-r1idfcx3|          3|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|      3|2024-09-13 06:59:48|6822499424790006|azuser2121_mml.lo...|    MERGE|{predicate -> [\"(...|NULL|{3562288518827637}|0911-073432-r1idfcx3|          2|WriteSerializable|        false|{numTargetRowsCop...|        NULL|Databricks-Runtim...|\n|      2|2024-09-13 06:59:15|6822499424790006|azuser2121_mml.lo...| OPTIMIZE|{predicate -> [],...|NULL|{3562288518827637}|0911-073432-r1idfcx3|          1|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Databricks-Runtim...|\n|      1|2024-09-13 06:59:10|6822499424790006|azuser2121_mml.lo...|    MERGE|{predicate -> [\"(...|NULL|{3562288518827637}|0911-073432-r1idfcx3|          0|WriteSerializable|        false|{numTargetRowsCop...|        NULL|Databricks-Runtim...|\n|      0|2024-09-13 06:46:37|6822499424790006|azuser2121_mml.lo...|    WRITE|{mode -> Overwrit...|NULL|{3562288518827637}|0911-073432-r1idfcx3|       NULL|WriteSerializable|        false|{numFiles -> 1, n...|        NULL|Databricks-Runtim...|\n+-------+-------------------+----------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[path: string]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 4\n",
    "# Advanced Features\n",
    "\n",
    "# DESCRIBE HISTORY\n",
    "spark.sql(\"DESCRIBE HISTORY sales_delta_table\").show()\n",
    "\n",
    "# VACUUM\n",
    "spark.sql(\"VACUUM sales_delta_table RETAIN 168 HOURS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08ce5e9e-1f23-486a-924b-c5a56b3a1d56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+--------+--------+-----+\n|OrderID| OrderDate|CustomerID| Product|Quantity|Price|\n+-------+----------+----------+--------+--------+-----+\n|   1001|2024-01-15|      C001|Widget A|      10|25.50|\n|   1002|2024-01-16|      C002|Widget B|       5|15.75|\n|   1003|2024-01-16|      C001|Widget C|       8|22.50|\n|   1004|2024-01-17|      C003|Widget A|      15|25.50|\n|   1005|2024-01-18|      C004|Widget D|       7|30.00|\n|   1006|2024-01-19|      C002|Widget B|       9|15.75|\n|   1007|2024-01-20|      C005|Widget C|      12|22.50|\n|   1008|2024-01-21|      C003|Widget A|      10|25.50|\n+-------+----------+----------+--------+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Task 5\n",
    "\n",
    "# Delta lake time travel\n",
    "spark.sql(\"SELECT * FROM sales_delta_table VERSION AS OF 0\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c74a737-8cd7-4f2b-bc9b-eb51e9260f1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging new customer data into the Delta Lake\nOptimizing\nVacuuming\n+----------+-------------+------+----------+\n|CustomerID| CustomerName|Region|SignupDate|\n+----------+-------------+------+----------+\n|      C001|     John Doe| North|2022-07-01|\n|      C003|Emily Johnson|  East|2021-11-20|\n|      C004|Michael Brown|  West|2022-12-05|\n|      C005|  Linda Davis| North|2023-03-10|\n|      C006|  Alex Wilson|  East|2024-02-01|\n|      C002|   Jane Smith|  West|2023-02-15|\n+----------+-------------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Building a reliable Data Lake with Delta Lake\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType\n",
    "# Schema enforcement\n",
    "customer_schema = StructType([\n",
    "    StructField(\"CustomerID\", StringType(), True),\n",
    "    StructField(\"CustomerName\", StringType(), True),\n",
    "    StructField(\"Region\", StringType(), True),\n",
    "    StructField(\"SignupDate\", StringType(), True)\n",
    "])\n",
    "\n",
    "customer_df = spark.read.json(\"dbfs:/FileStore/customer_data.json\",schema=customer_schema)\n",
    "\n",
    "customer_df.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/FileStore/customers_delta\")\n",
    "\n",
    "new_customer_data = [\n",
    "    (\"C006\", \"Alex Wilson\", \"East\", \"2024-02-01\"),\n",
    "    (\"C002\", \"Jane Smith\", \"West\", \"2023-02-15\") \n",
    "]\n",
    "\n",
    "new_customer_df = spark.createDataFrame(new_customer_data, schema=customer_schema)\n",
    "new_customer_df.createOrReplaceTempView(\"new_customer_data\")\n",
    "\n",
    "customer_df = spark.read.format(\"delta\").load(\"dbfs:/FileStore/customers_delta\")\n",
    "customer_df.createOrReplaceTempView(\"customer_delta_table\")\n",
    "\n",
    "print(\"Merging new customer data into the Delta Lake\")\n",
    "merge_query = f\"\"\"\n",
    "MERGE INTO customer_delta_table AS target\n",
    "USING new_customer_data AS source\n",
    "ON target.CustomerID = source.CustomerID\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET\n",
    "    CustomerName = source.CustomerName,\n",
    "    Region = source.Region,\n",
    "    SignupDate = source.SignupDate\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (CustomerID, CustomerName, Region, SignupDate)\n",
    "  VALUES (source.CustomerID, source.CustomerName, source.Region, source.SignupDate)\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(merge_query)\n",
    "\n",
    "# Optimize\n",
    "print(\"Optimizing\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS customer_delta_table1 USING DELTA AS SELECT * FROM customer_delta_table\")\n",
    "\n",
    "spark.sql(\"OPTIMIZE customer_delta_table1 ZORDER BY (Region)\")\n",
    "\n",
    "# Vacuum\n",
    "print(\"Vacuuming\")\n",
    "spark.sql(\"VACUUM customer_delta_table1 RETAIN 168 HOURS\")\n",
    "\n",
    "# Display the delta lake\n",
    "spark.sql(\"SELECT * FROM customer_delta_table1\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "delta-lake-exercise",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
